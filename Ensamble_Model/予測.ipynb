{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "505b4ee6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jjzha/jobbert-base-cased_h h Predicting...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at jjzha/jobbert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertModel were not initialized from the model checkpoint at jjzha/jobbert-base-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at jjzha/jobbert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertModel were not initialized from the model checkpoint at jjzha/jobbert-base-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at jjzha/jobbert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertModel were not initialized from the model checkpoint at jjzha/jobbert-base-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at jjzha/jobbert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertModel were not initialized from the model checkpoint at jjzha/jobbert-base-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7db900f54f3d42bcb8fd2839e37bdb4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1517 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jjzha/jobbert-base-cased_h m Predicting...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at jjzha/jobbert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertModel were not initialized from the model checkpoint at jjzha/jobbert-base-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at jjzha/jobbert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertModel were not initialized from the model checkpoint at jjzha/jobbert-base-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at jjzha/jobbert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertModel were not initialized from the model checkpoint at jjzha/jobbert-base-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at jjzha/jobbert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertModel were not initialized from the model checkpoint at jjzha/jobbert-base-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da54a7c18acd42458610f5fd48d4c8e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1517 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jjzha/jobbert-base-cased_h t Predicting...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at jjzha/jobbert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertModel were not initialized from the model checkpoint at jjzha/jobbert-base-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at jjzha/jobbert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertModel were not initialized from the model checkpoint at jjzha/jobbert-base-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at jjzha/jobbert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertModel were not initialized from the model checkpoint at jjzha/jobbert-base-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at jjzha/jobbert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertModel were not initialized from the model checkpoint at jjzha/jobbert-base-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b0db549e8a44e6ba95f971285fa2596",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1517 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jjzha/jobbert-base-cased_h ht Predicting...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at jjzha/jobbert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertModel were not initialized from the model checkpoint at jjzha/jobbert-base-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at jjzha/jobbert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertModel were not initialized from the model checkpoint at jjzha/jobbert-base-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at jjzha/jobbert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertModel were not initialized from the model checkpoint at jjzha/jobbert-base-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at jjzha/jobbert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertModel were not initialized from the model checkpoint at jjzha/jobbert-base-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c567ef8c74154133953954dcfe8dd365",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1517 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os, gc, sys\n",
    "import re\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import lightgbm as lgb\n",
    "from catboost import CatBoost, Pool\n",
    "import pulp\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from transformers import AutoTokenizer, AutoModel, AdamW\n",
    "import nlp\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "def seed_everything(seed):\n",
    "    \"\"\"\n",
    "    GPU+Pytorchを使用する場合の再現性確保のための関数.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    seed: int\n",
    "        固定するシードの値.\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "def np_rounder(x):\n",
    "    \"\"\"\n",
    "    numpyの四捨五入用関数.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    x: np.array[float]\n",
    "\n",
    "    Returns:\n",
    "    ----------\n",
    "    (int_array + float_array).astype(int): np.array[int]\n",
    "    \"\"\"\n",
    "    int_array = x // 1\n",
    "    float_array = x % 1\n",
    "    float_array[float_array<0.5] = 0\n",
    "    float_array[float_array>=0.5] = 1\n",
    "    return (int_array + float_array).astype(int)\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    尤度を確率に変換する関数.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    x: np.array[float]\n",
    "\n",
    "    Returns:\n",
    "    1 / (1+np.exp(-x)) : np.array[float]\n",
    "    \"\"\"\n",
    "    return 1 / (1+np.exp(-x))\n",
    "\n",
    "\n",
    "def make_dataset(df, tokenizer, device, model_name):\n",
    "    \"\"\"\n",
    "    NLPモデル用のデータセットを作成するための関数.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    df: pd.DataFrame\n",
    "        モデル用のデータセット.\n",
    "    tokenizer: transformers.AutoTokenizer.from_pretrained\n",
    "        モデル用のtokenizer.\n",
    "    device: str\n",
    "        使用するデバイス. \"cpu\" or \"cuda\".\n",
    "    model_name: str\n",
    "        使用するモデルの名前.\n",
    "    \n",
    "    Returns:\n",
    "    ----------\n",
    "    dataset: nlp.Dataset.from_pandas\n",
    "        NLP用のデータセット.\n",
    "    \"\"\"\n",
    "    dataset = nlp.Dataset.from_pandas(df)\n",
    "    dataset = dataset.map(\n",
    "        lambda example: tokenizer(example[params.TEXT_COL],\n",
    "                                  padding=\"max_length\",\n",
    "                                  truncation=True,\n",
    "                                  max_length=params.MAX_TOKEN_LEN))\n",
    "    if not model_name in [\"roberta-base\", \"distilbert-base-uncased\"]:\n",
    "        dataset.set_format(type='torch', \n",
    "                           columns=['input_ids', 'token_type_ids', 'attention_mask', 'labels'], \n",
    "                           device=device)\n",
    "    else:\n",
    "        dataset.set_format(type='torch', \n",
    "                           columns=['input_ids', 'attention_mask', 'labels'], \n",
    "                           device=device)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def predict_lgb(X_test, n_folds=4):\n",
    "    \"\"\"\n",
    "    lightgbm予測用関数.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    X_test: pd.DataFrame\n",
    "        予測用データセット.\n",
    "    n_folds: int\n",
    "        予測時のFold数. 訓練時のFold数より大きくしないこと.\n",
    "    \n",
    "    Returns:\n",
    "    ----------\n",
    "    y_pred: np.array[float]\n",
    "        予測した尤度.\n",
    "    \"\"\"\n",
    "    y_pred = np.zeros((X_test.shape[0], params.NUM_CLASS), dtype='float32')\n",
    "    for fold in range(n_folds):\n",
    "        model = pickle.load(open(params.MODELS_DIR+\"lgb_fold{}.lgbmodel\".format(fold), \"rb\"))\n",
    "        y_pred += model.predict(X_test, num_iteration=model.best_iteration) / n_folds\n",
    "    return y_pred\n",
    "\n",
    "\n",
    "def predict_ctb(X_test, n_folds=4):\n",
    "    \"\"\"\n",
    "    catboost予測用関数.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    X_test: pd.DataFrame\n",
    "        予測用データセット.\n",
    "    n_folds: int\n",
    "        予測時のFold数. 訓練時のFold数より大きくしないこと.\n",
    "    \n",
    "    Returns:\n",
    "    ----------\n",
    "    y_pred: np.array[float]\n",
    "        予測した尤度.\n",
    "    \"\"\"\n",
    "    y_pred = np.zeros((X_test.shape[0], params.NUM_CLASS), dtype='float32')\n",
    "    for fold in range(n_folds):\n",
    "        model = pickle.load(open(params.MODELS_DIR+\"ctb_fold{}.ctbmodel\".format(fold), \"rb\"))\n",
    "        y_pred += model.predict(X_test) / n_folds\n",
    "    return y_pred\n",
    "\n",
    "\n",
    "def predict_nlp(model_name, typ, file_path):\n",
    "    \"\"\"\n",
    "    nlp予測用関数.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    model_name: str\n",
    "        使用するモデルの名前.\n",
    "    type: str\n",
    "        使用する特徴量の部分.\n",
    "    file_path: str\n",
    "        予測するデータセットのパス.\n",
    "    \n",
    "    Returns:\n",
    "    ----------\n",
    "    preds: np.array[float]\n",
    "        予測した尤度.\n",
    "    \"\"\"\n",
    "    models = []\n",
    "    for fold in range(params.NUM_SPLITS):\n",
    "        model = Classifier(model_name, typ)\n",
    "        model.load_state_dict(torch.load(params.MODELS_DIR + f\"best_{model_name}_{typ}_{fold}.pth\"))\n",
    "        model.to(params.DEVICE)\n",
    "        model.eval()\n",
    "        models.append(model)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    test_df = pd.read_csv(file_path)\n",
    "    test_df[\"labels\"] = -1\n",
    "    test_dataset = make_dataset(test_df, tokenizer, params.DEVICE, model_name)\n",
    "    test_dataloader = torch.utils.data.DataLoader(\n",
    "        test_dataset, batch_size=params.VALID_BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        final_output = []\n",
    "        preds = []\n",
    "        for batch in test_dataloader:            \n",
    "            if len(batch.values())==4:\n",
    "                attention_mask, input_ids, labels, token_type_ids = batch.values()\n",
    "            else:\n",
    "                attention_mask, input_ids, labels = batch.values()\n",
    "                token_type_ids = None\n",
    "            pred = np.zeros((labels.shape[0], params.NUM_CLASS))\n",
    "            for model in models:\n",
    "                pred += model(input_ids, attention_mask, token_type_ids).cpu().numpy()\n",
    "            preds += (pred/params.NUM_SPLITS).tolist()\n",
    "    return preds\n",
    "\n",
    "\n",
    "def hack(prob):\n",
    "    \"\"\"\n",
    "    from: https://signate.jp/competitions/281/discussions/20200816040343-8180\n",
    "    尤度最大化用関数.\n",
    "\n",
    "    Parameters:\n",
    "    ------------\n",
    "    prob: np.array[float]\n",
    "        予測した確率.\n",
    "    \n",
    "    Returns:\n",
    "    ------------\n",
    "    x_ast.argmax(axis=1): np.array[int]\n",
    "        予測したラベル.\n",
    "    \"\"\"\n",
    "    logp = np.log(prob + 1e-16)\n",
    "    N = prob.shape[0]\n",
    "    K = prob.shape[1]\n",
    "    m = pulp.LpProblem('Problem', pulp.LpMaximize)\n",
    "    x = pulp.LpVariable.dicts('x', [(i, j) for i in range(N) for j in range(K)], 0, 1, pulp.LpBinary)\n",
    "    log_likelihood = pulp.lpSum([x[(i, j)] * logp[i, j] for i in range(N) for j in range(K)])\n",
    "    m += log_likelihood\n",
    "    for i in range(N):\n",
    "        m += pulp.lpSum([x[(i, k)] for k in range(K)]) == 1\n",
    "    for k in range(K):\n",
    "        m += pulp.lpSum([x[(i, k)] for i in range(N)]) == params.N_CLASSES[k]\n",
    "    m.solve()\n",
    "    assert m.status == 1\n",
    "    x_ast = np.array([[int(x[(i, j)].value()) for j in range(K)] for i in range(N)])\n",
    "    return x_ast.argmax(axis=1)\n",
    "\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    \"\"\"\n",
    "    NLPタスク分類用モデルクラス.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    model_name: str\n",
    "        使用するモデルの名前.\n",
    "    typ: str\n",
    "        NLPモデルから特徴量を取る位置.\n",
    "    num_classes: int\n",
    "        学習するデータのクラス数.\n",
    "    \"\"\"\n",
    "    def __init__(self, model_name, typ, num_classes=3):\n",
    "        super().__init__()\n",
    "\n",
    "        self.name = model_name\n",
    "        self.typ = typ\n",
    "        if model_name in [\"albert-large-v2\", \"xlm-mlm-ende-1024\"]:\n",
    "            nodes = 1024\n",
    "        else:\n",
    "            nodes = 768\n",
    "\n",
    "        self.bert = AutoModel.from_pretrained(model_name)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        if typ != \"ht\":\n",
    "            self.linear = nn.Linear(nodes, num_classes)\n",
    "        else:\n",
    "            self.linear = nn.Linear(nodes*2, num_classes)\n",
    "        nn.init.normal_(self.linear.weight, std=0.02)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids=None):\n",
    "        if self.name in [\"bert-base-uncased\", \"albert-base-v2\",\"jjzha/jobbert-base-cased\"]:\n",
    "            output, _ = self.bert(\n",
    "                input_ids = input_ids,\n",
    "                attention_mask = attention_mask,\n",
    "                token_type_ids = token_type_ids,\n",
    "                return_dict=False)\n",
    "            #output = output[:, 0, :]\n",
    "        elif self.name in [\"xlnet-base-cased\", \"xlm-mlm-ende-1024\"]:\n",
    "            output = self.bert(\n",
    "                input_ids = input_ids,\n",
    "                attention_mask = attention_mask,\n",
    "                token_type_ids = token_type_ids)\n",
    "            output = output[0]\n",
    "            #output = output[:, 0, :]\n",
    "        elif self.name in [\"roberta-base\", 'microsoft/deberta-base',\"microsoft/deberta-v3-base\",\"distilbert-base-uncased\"]:\n",
    "            output = self.bert(\n",
    "                input_ids = input_ids,\n",
    "                attention_mask = attention_mask,\n",
    "                 return_dict=False\n",
    "                )\n",
    "            output = output[0]\n",
    "            #output = output[:, 0, :]\n",
    "        \n",
    "        if self.typ == \"h\":\n",
    "            output = output[:, 0, :]\n",
    "        elif self.typ == \"m\":\n",
    "            output = torch.mean(output, dim=1)\n",
    "        elif self.typ == \"t\" or self.typ==\"FRt\" or self.typ==\"DEt\":\n",
    "            output = output[:, -1, :]\n",
    "        elif self.typ ==  \"ht\":\n",
    "            output = torch.cat((output[:, 0, :], output[:, -1, :]), dim=-1)\n",
    "        else:\n",
    "            output = output[:, 0, :]\n",
    "        output = self.dropout(output)\n",
    "        output = self.linear(output)\n",
    "        return output\n",
    "\n",
    "\n",
    "class Parameters(object):\n",
    "    \"\"\"\n",
    "    パラメータ管理用クラス.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.SEED = 2020\n",
    "        # コードのパス. os.getcwd()が動かない場合はstrで直接渡す.\n",
    "        #BASE_PATH = \"C:/StudentCup2020/2nd/\"\n",
    "        self.BASE_PATH = os.getcwd() + '/'\n",
    "        self.TEST_FILE = self.BASE_PATH + \"data/test.csv\"\n",
    "        self.TEXT_COL = \"description\"\n",
    "        self.TARGET = \"jobflag\"\n",
    "        self.NUM_CLASS = 3\n",
    "        \n",
    "        self.LGB_TEST_FILE = self.BASE_PATH+\"data/lgb_test.csv\"\n",
    "        self.OUTPUT_PATH = self.BASE_PATH + \"outputs/\"\n",
    "        \n",
    "        self.TRAIN_WEIGHT = np.array([0.3,0.3,0.35])\n",
    "        self.TEST_WEIGHT = np.array([0.3,0.3,0.35])\n",
    "\n",
    "        self.CLASS_WEIGHT = self.TEST_WEIGHT / self.TRAIN_WEIGHT\n",
    "        self.CLASS_WEIGHT /= sum(self.CLASS_WEIGHT)\n",
    "        self.CLASS_WEIGHT_TENSOR = torch.tensor(self.CLASS_WEIGHT).cuda()\n",
    "\n",
    "        len_test = len(pd.read_csv(self.TEST_FILE))\n",
    "        self.N_CLASSES = np_rounder(len_test*self.TEST_WEIGHT).tolist()\n",
    "        while sum(self.N_CLASSES) < len_test:\n",
    "            diff = np.abs(0.5 - len_test*self.TEST_WEIGHT%1)\n",
    "            self.N_CLASSES[np.argmin(diff)] += 1\n",
    "        while sum(self.N_CLASSES) > len_test:\n",
    "            diff = np.abs(0.5 - len_test*self.TEST_WEIGHT%1)\n",
    "            self.N_CLASSES[np.argmin(diff)] -= 1\n",
    "\n",
    "        self.DEVICE = \"cuda\"\n",
    "        self.MODELS_DIR = self.BASE_PATH + \"models/\"\n",
    "        self.NUM_SPLITS = 4\n",
    "        \n",
    "        self.VALID_BATCH_SIZE = 128\n",
    "        self.MAX_TOKEN_LEN = 128\n",
    "params = Parameters()\n",
    "\n",
    "\n",
    "def main(params):\n",
    "    \"\"\"\n",
    "    # --- lightgbm --- #\n",
    "    print(\"LightGBM Predicting...\")\n",
    "    X_test = pd.read_csv(params.LGB_TEST_FILE)\n",
    "    y_pred = predict_lgb(X_test, n_folds=params.NUM_SPLITS)\n",
    "    np.save(params.OUTPUT_PATH+\"lgb_yprd\", y_pred)\n",
    "    \n",
    "    # --- catboost --- #\n",
    "    print(\"CatBoost Predicting...\")\n",
    "    test = pd.read_csv(params.TEST_FILE)\n",
    "    col = [c for c in test.columns if c not in ['id', params.TARGET]]\n",
    "    X_test = test[col]\n",
    "    y_pred = predict_ctb(X_test, n_folds=params.NUM_SPLITS)\n",
    "    np.save(params.OUTPUT_PATH+\"cat_yprd\", y_pred)\n",
    "    \n",
    "    # --- roberta --- #\n",
    "    model_name = \"roberta-base\"\n",
    "    typs = [\"h\", \"m\", \"t\", \"ht\"]\n",
    "    for typ in typs:\n",
    "        print(\"Robert {} Predicting...\".format(typ))\n",
    "        preds = predict_nlp(model_name, typ, params.TEST_FILE)\n",
    "        np.save(params.OUTPUT_PATH+model_name+\"_\"+typ+\"_yprd\", preds)\n",
    "    \n",
    "    # --- deberta --- #\n",
    "    model_name = \"microsoft/deberta-base\"\n",
    "    typs = [\"h\", \"m\", \"t\", \"ht\"]\n",
    "    for typ in typs:\n",
    "        print(\"debert {} Predicting...\".format(typ))\n",
    "        preds = predict_nlp(model_name, typ, params.TEST_FILE)\n",
    "        np.save(params.OUTPUT_PATH+model_name+\"_\"+typ+\"_yprd\", preds)\n",
    "        \n",
    "    # --- deberta-v3 --- #\n",
    "    model_name = \"microsoft/deberta-v3-base\"\n",
    "    typs = [\"h\", \"m\", \"t\", \"ht\"]\n",
    "    for typ in typs:\n",
    "        print(\"deberta-v3 {} Predicting...\".format(typ))\n",
    "        preds = predict_nlp(model_name, typ, params.TEST_FILE)\n",
    "        np.save(params.OUTPUT_PATH+model_name+\"_\"+typ+\"_yprd\", preds)\n",
    "    \n",
    "    # --- albert --- #\n",
    "    model_name = \"albert-base-v2\"\n",
    "    typs = [\"h\", \"m\", \"t\", \"ht\"]\n",
    "    for typ in typs:\n",
    "        print(\"albert-base-v2 {} Predicting...\".format(typ))\n",
    "        preds = predict_nlp(model_name, typ, params.TEST_FILE)\n",
    "        np.save(params.OUTPUT_PATH+model_name+\"_\"+typ+\"_yprd\", preds)\n",
    "    \n",
    "    # --- distilbert-base-uncased --- #\n",
    "    model_name = \"distilbert-base-uncased\"\n",
    "    typs = [\"h\", \"m\", \"t\", \"ht\"]\n",
    "    for typ in typs:\n",
    "        print(\"distilbert-base-uncased {} Predicting...\".format(typ))\n",
    "        preds = predict_nlp(model_name, typ, params.TEST_FILE)\n",
    "        np.save(params.OUTPUT_PATH+model_name+\"_\"+typ+\"_yprd\", preds)\n",
    "    \n",
    "    # --- xlnet-base-cased --- #\n",
    "    model_name = \"xlnet-base-cased\"\n",
    "    typs = [\"h\", \"m\", \"t\", \"ht\"]\n",
    "    for typ in typs:\n",
    "        print(\"xlnet-base-cased {} Predicting...\".format(typ))\n",
    "        preds = predict_nlp(model_name, typ, params.TEST_FILE)\n",
    "        np.save(params.OUTPUT_PATH+model_name+\"_\"+typ+\"_yprd\", preds)\n",
    "    \n",
    "    # --- bert-base-uncased --- #\n",
    "    model_name = \"bert-base-uncased\"\n",
    "    typs = [\"h\", \"m\", \"t\", \"ht\"]\n",
    "    for typ in typs:\n",
    "        print(\"bert-base-uncased {} Predicting...\".format(typ))\n",
    "        preds = predict_nlp(model_name, typ, params.TEST_FILE)\n",
    "        np.save(params.OUTPUT_PATH+model_name+\"_\"+typ+\"_yprd\", preds)\n",
    "    \"\"\"\n",
    "    # --- jjzha/jobbert-base-cased_h --- #\n",
    "    model_name = \"jjzha/jobbert-base-cased\"\n",
    "    typs = [\"h\", \"m\", \"t\", \"ht\"]\n",
    "    for typ in typs:\n",
    "        print(\"jjzha/jobbert-base-cased_h {} Predicting...\".format(typ))\n",
    "        preds = predict_nlp(model_name, typ, params.TEST_FILE)\n",
    "        np.save(params.OUTPUT_PATH+model_name+\"_\"+typ+\"_yprd\", preds)\n",
    "    # --- ensemble --- #\n",
    "    model_names = [\"lgb\",\n",
    "                   \"cat\",\n",
    "                    \"jjzha/jobbert-base-cased_h\",\n",
    "                   \"jjzha/jobbert-base-cased_m\",\n",
    "                   \"jjzha/jobbert-base-cased_t\",\n",
    "                   \"jjzha/jobbert-base-cased_ht\",\n",
    "                    \"albert-base-v2_h\",\n",
    "                   \"albert-base-v2_m\",\n",
    "                  \"albert-base-v2_t\",\n",
    "                   \"albert-base-v2_ht\",\n",
    "                   \"roberta-base_h\",\n",
    "                   \"roberta-base_m\",\n",
    "                   \"roberta-base_t\",\n",
    "                   \"roberta-base_ht\",\n",
    "                   \"distilbert-base-uncased_h\",\n",
    "                   \"distilbert-base-uncased_m\",\n",
    "                   \"distilbert-base-uncased_t\",\n",
    "                   \"distilbert-base-uncased_ht\",\n",
    "                   \"xlnet-base-cased_h\",\n",
    "                   \"xlnet-base-cased_m\",\n",
    "                   \"xlnet-base-cased_t\",\n",
    "                   \"xlnet-base-cased_ht\",\n",
    "                   \"bert-base-uncased_h\",\n",
    "                   \"bert-base-uncased_m\",\n",
    "                   \"bert-base-uncased_t\",\n",
    "                   \"bert-base-uncased_ht\",\n",
    "                   \"microsoft/deberta-base_h\",\n",
    "                   \"microsoft/deberta-base_m\", \n",
    "                   \"microsoft/deberta-base_t\",\n",
    "                   \"microsoft/deberta-base_ht\",\n",
    "                  \"microsoft/deberta-v3-base_h\",\n",
    "                   \"microsoft/deberta-v3-base_m\", \n",
    "                   \"microsoft/deberta-v3-base_t\",\n",
    "                   \"microsoft/deberta-v3-base_ht\"]\n",
    "    test = pd.read_csv(params.TEST_FILE)\n",
    "    y_pred = np.zeros((test.shape[0], 3, len(model_names)))\n",
    "    for i, model_name in enumerate(model_names):\n",
    "        yprd = np.load(params.OUTPUT_PATH+model_name+\"_yprd.npy\")\n",
    "        y_pred[:, :, i] = yprd\n",
    "    best_w = np.load(params.OUTPUT_PATH+\"config_ensemble_bestw.npy\")\n",
    "    best_cw = np.load(params.OUTPUT_PATH+\"config_ensemble_bestcw.npy\")\n",
    "    test_pred = np.average(y_pred, axis=2, weights=best_w)\n",
    "    test_pred = test_pred * best_cw\n",
    "\n",
    "    # --- post processing --- #\n",
    "    test_pred = sigmoid(test_pred)\n",
    "    test_pred = test_pred / np.sum(test_pred, axis=1).reshape(test.shape[0], -1)\n",
    "    test_pred = hack(test_pred) + 1\n",
    "    \n",
    "    test = pd.read_csv(params.TEST_FILE)\n",
    "    submit = pd.DataFrame({'index':test['id'], 'pred':test_pred})\n",
    "    submit.to_csv(params.BASE_PATH+\"data/submission.csv\", index=False, header=False)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    seed_everything(params.SEED)\n",
    "    if \"models\" not in os.listdir(params.BASE_PATH):\n",
    "        os.mkdir(params.BASE_PATH + \"models/\")\n",
    "    if \"outputs\" not in os.listdir(params.BASE_PATH):\n",
    "        os.mkdir(params.BASE_PATH + \"outputs/\")\n",
    "    main(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "89f060a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1516</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1517</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1518</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1519</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1520</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1512</th>\n",
       "      <td>3028</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1513</th>\n",
       "      <td>3029</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1514</th>\n",
       "      <td>3030</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1515</th>\n",
       "      <td>3031</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1516</th>\n",
       "      <td>3032</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1517 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0  1\n",
       "0     1516  1\n",
       "1     1517  4\n",
       "2     1518  3\n",
       "3     1519  4\n",
       "4     1520  3\n",
       "...    ... ..\n",
       "1512  3028  4\n",
       "1513  3029  1\n",
       "1514  3030  3\n",
       "1515  3031  4\n",
       "1516  3032  1\n",
       "\n",
       "[1517 rows x 2 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "No_ML = pd.read_csv(\"./data/submission.csv\",header=None)\n",
    "re_labels = {1:1 , 2:3 , 3:4}\n",
    "No_ML[1] = No_ML[1].map(re_labels)\n",
    "\n",
    "\n",
    "ML = pd.read_csv(\"./data/Bert_submission_cv0694797761.csv\",header=None)\n",
    "\n",
    "ML = ML[ML[1]==2]\n",
    "print(len(ML))\n",
    "\n",
    "pred = pd.concat([ML,No_ML],axis=0)\n",
    "pred = pred.drop_duplicates(subset=0)\n",
    "pred = pred.sort_values(0).reset_index(drop=True)\n",
    "            \n",
    "pred.to_csv(\"./data/submission_bert_ens.csv\", index=False, header=False)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "44b58040",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3    528\n",
       "4    505\n",
       "1    414\n",
       "2     70\n",
       "Name: 1, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_1 = pd.read_csv(\"./data/Bert_submission_cv0694797761.csv\",header=None)\n",
    "df_2 = pd.read_csv(\"./data/submission_bert_ens.csv\",header=None)\n",
    "\n",
    "df_1[1].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3c8b301d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4    530\n",
       "1    466\n",
       "3    451\n",
       "2     70\n",
       "Name: 1, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_2[1].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d204a0a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "re_labels = {1: \"DS\", 2: \"ML\", 3:\"SE\", 4:\"Cons\"}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
