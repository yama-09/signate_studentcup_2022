{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eac9d68d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, gc, sys\n",
    "import re\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "from googletrans import Translator\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import lightgbm as lgb\n",
    "from catboost import CatBoost, Pool\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from transformers import AutoTokenizer, AutoModel, AdamW\n",
    "import nlp\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def seed_everything(seed):\n",
    "    \"\"\"\n",
    "    GPU+Pytorchを使用する場合の再現性確保のための関数.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    seed: int\n",
    "        固定するシードの値.\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "\n",
    "def del_space(x):\n",
    "    \"\"\"\n",
    "    クリーニング用の関数.\n",
    "    余計な空白を除去する.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    x: str\n",
    "        クリーニングしたいテキスト\n",
    "    \n",
    "    Returns:\n",
    "    -----------\n",
    "    x: str\n",
    "        クリーニングしたテキスト\n",
    "    \"\"\"\n",
    "    while '  ' in x:\n",
    "        x = x.replace('  ', ' ')\n",
    "    return x\n",
    "\n",
    "\n",
    "def cleaning(texts):\n",
    "    \"\"\"from https://signate.jp/competitions/281/tutorials/17\n",
    "    SGINATEチュートリアルから参照.\n",
    "    データクリーニング用の関数.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    texts: List[str]\n",
    "        クリーニングしたいテキストのリスト.\n",
    "\n",
    "    Returns:\n",
    "    -----------\n",
    "    clean_texts: List[str]\n",
    "        クリーニングしたテキストのリスト.\n",
    "    \"\"\"\n",
    "    clean_texts = []\n",
    "    stemmer = PorterStemmer()\n",
    "    for text in texts:\n",
    "        clean_punc = re.sub(r'[^a-zA-Z]', ' ', text)\n",
    "        clean_short_tokenized = [word for word in clean_punc.split() if len(word) > 3]\n",
    "        clean_normalize = [stemmer.stem(word) for word in clean_short_tokenized]\n",
    "        clean_text = ' '.join(clean_normalize)\n",
    "        clean_texts.append(clean_text)\n",
    "    return clean_texts\n",
    "\n",
    "\n",
    "def feature_extraction_vc(df, bottom_thld=0.0025, upper_thld=0.5):\n",
    "    \"\"\"\n",
    "    Countベースのテキスト特徴量抽出関数.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df: pd.DataFrame\n",
    "        特徴量抽出をしたいデータフレーム.\n",
    "    bottom_thld, upper_thld: float, float\n",
    "        使用する特徴量の出現頻度の下限と上限.\n",
    "        \n",
    "    Returns:\n",
    "    -----------\n",
    "    voc_df: pd.DataFrame\n",
    "        Countベースで特徴抽出したデータフレーム\n",
    "    \"\"\"\n",
    "    vc = CountVectorizer()\n",
    "    df = vc.fit_transform(df[params.TEXT_COL])\n",
    "    voc_df = pd.DataFrame(df.toarray(), columns=vc.get_feature_names())\n",
    "    use_cols = []\n",
    "    for col in voc_df.columns:\n",
    "        if voc_df.shape[0]*bottom_thld<voc_df[col].sum()<voc_df.shape[0]*upper_thld:\n",
    "            use_cols.append(col)\n",
    "    voc_df = voc_df[use_cols]\n",
    "    voc_cols = {col:col+'_voc' for col in voc_df.columns}\n",
    "    voc_df = voc_df.rename(columns=voc_cols)\n",
    "    return voc_df\n",
    "\n",
    "\n",
    "def feature_extraction_tfidf(df, bottom_thld=0.9):\n",
    "    \"\"\"\n",
    "    tfidfベースのテキスト特徴量抽出関数.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df: pd.DataFrame\n",
    "        特徴量抽出をしたいデータフレーム.\n",
    "    bottom_thld: float\n",
    "        使用する特徴量の標準偏差の下限.\n",
    "\n",
    "    Returns:\n",
    "    -----------\n",
    "    tdidf_df: pd.DataFrame\n",
    "        tfidfベースで特徴抽出したデータフレーム\n",
    "    \"\"\"\n",
    "    tfidf = TfidfVectorizer()\n",
    "    df = tfidf.fit_transform(df)\n",
    "    tfidf_df = pd.DataFrame(df.toarray(), columns=tfidf.get_feature_names())\n",
    "    use_cols = []\n",
    "    thld = np.percentile(tfidf_df.std().values, bottom_thld*100)\n",
    "    for col in tfidf_df.columns:\n",
    "        if thld < tfidf_df[col].std():\n",
    "            use_cols.append(col)\n",
    "    tfidf_df = tfidf_df[use_cols]\n",
    "    tfidf_cols = {col:col+'_tfidf' for col in tfidf_df.columns}\n",
    "    tfidf_df = tfidf_df.rename(columns=tfidf_cols)\n",
    "    return tfidf_df\n",
    "\n",
    "\n",
    "def preprocessing_lgb(vc_btm_thld=0.0025, vc_upr_thld=0.5, tfidf_thld=0.9):\n",
    "    \"\"\"\n",
    "    lightgbm用の前処理関数.\n",
    "\n",
    "    Parameters:\n",
    "    ------------\n",
    "    vc_btm_thld: float\n",
    "        使用する特徴量の出現頻度の下限.\n",
    "    vc_upr_thld: float\n",
    "        使用する特徴量の出現頻度の上限.\n",
    "    tfidf_thld: float\n",
    "        tfidfで使用する特徴量の標準偏差の下限.\n",
    "\n",
    "    Returns:\n",
    "    ------------\n",
    "    train: pd.DataFrame\n",
    "        訓練用データフレーム.\n",
    "    test: pd.DataFrame\n",
    "        テスト用データフレーム.\n",
    "    \"\"\"\n",
    "    train = pd.read_csv(params.TRAIN_FILE)\n",
    "    test = pd.read_csv(params.TEST_FILE)\n",
    "    test[params.TARGET] = -1\n",
    "\n",
    "    df = pd.concat([train, test]).reset_index(drop=True)\n",
    "    df[params.TEXT_COL] = df[params.TEXT_COL].apply(lambda x: del_space(x))\n",
    "    \n",
    "    df['description'] = cleaning(df['description'])\n",
    "    voc_df = feature_extraction_vc(df, vc_btm_thld, vc_upr_thld)\n",
    "    tfidf_df = feature_extraction_tfidf(df, tfidf_thld)\n",
    "    df = pd.concat([pd.concat([train,test]).reset_index(drop=True), voc_df, tfidf_df], axis=1)\n",
    "    train = df.iloc[:train.shape[0], :]\n",
    "    test = df.iloc[train.shape[0]:, :]\n",
    "    \n",
    "    del voc_df, tfidf_df\n",
    "    gc.collect()\n",
    "\n",
    "    col = [c for c in train.columns if c not in ['id', params.TEXT_COL]]\n",
    "    train = train[col]\n",
    "    test = test[col].drop([params.TARGET], axis=1)\n",
    "    return train, test\n",
    "\n",
    "\n",
    "def get_train_data(params):\n",
    "    \"\"\"訓練用のデータフレームを返す.\n",
    "    \"\"\"\n",
    "    return main(params)[0]\n",
    "\n",
    "def get_test_data(params):\n",
    "    \"\"\"テスト用のデータフレームを返す.\n",
    "    \"\"\"\n",
    "    return main(params)[1]\n",
    "\n",
    "def main(params):\n",
    "    #train_fr, test_fr, train_de, test_de = get_googletranslate(params)\n",
    "\n",
    "    lgb_train, lgb_test = preprocessing_lgb()\n",
    "    lgb_train.to_csv(params.BASE_PATH+\"data/lgb_train.csv\", index=False)\n",
    "    lgb_test.to_csv(params.BASE_PATH+\"data/lgb_test.csv\", index=False)\n",
    "\n",
    "    train = pd.read_csv(params.TRAIN_FILE)\n",
    "    test = pd.read_csv(params.TEST_FILE)\n",
    "    return (train,lgb_train), (test,lgb_test)\n",
    "\n",
    "\n",
    "class Parameters(object):\n",
    "    \"\"\"\n",
    "    パラメータ管理用のクラス.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.SEED = 2020\n",
    "        # コードのパス. os.getcwd()が動かない場合はstrで直接渡す.\n",
    "        #BASE_PATH = \"C:/StudentCup2020/\"\n",
    "        self.BASE_PATH = os.getcwd() + '/'\n",
    "        self.TRAIN_FILE = self.BASE_PATH + \"data/train.csv\"\n",
    "        self.TEST_FILE = self.BASE_PATH + \"data/test.csv\"\n",
    "        self.TEXT_COL = \"description\"\n",
    "        self.TARGET = \"jobflag\"\n",
    "        self.NUM_CLASS = 4\n",
    "params = Parameters()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    seed_everything(params.SEED)\n",
    "    if \"models\" not in os.listdir(params.BASE_PATH):\n",
    "        os.mkdir(params.BASE_PATH + \"models/\")\n",
    "    main(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98bd303a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
