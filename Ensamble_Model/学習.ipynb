{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "ec9090cc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 100000/100000 [01:55<00:00, 863.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Ensemble Score: 0.8393948614719628\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os, gc, sys\n",
    "import re\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import lightgbm as lgb\n",
    "from catboost import CatBoost, Pool\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from transformers import AutoTokenizer, AutoModel, AdamW\n",
    "import nlp\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "def seed_everything(seed):\n",
    "    \"\"\"\n",
    "    GPU+Pytorchを使用する場合の再現性確保のための関数.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    seed: int\n",
    "        固定するシードの値.\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "def metric_f1(labels, preds):\n",
    "    \"\"\"\n",
    "    クラスごとに重みづけしたF1評価関数.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    labels: np.array\n",
    "        正解ラベル.\n",
    "    preds: np.array\n",
    "        予測ラベル. 予測確率ではないことに注意.\n",
    "\n",
    "    Returns:\n",
    "    -----------\n",
    "    score: params.CLASS_WEIGHTで重みづけされたF1スコア.\n",
    "    \"\"\"\n",
    "    return f1_score(labels, preds, average=None) @ params.CLASS_WEIGHT\n",
    "\n",
    "\n",
    "def metric_f1_lgb(preds, data):\n",
    "    \"\"\"\n",
    "    lightgbmのためのF1評価関数.\n",
    "    詳細はlightgbmのドキュメント参照.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    preds: np.array\n",
    "        予測値, flattenされているのでreshapeする必要あり.\n",
    "    data: lightgbm.Dataset\n",
    "        学習データ.\n",
    "    \n",
    "    Returns:\n",
    "    -----------\n",
    "    \"metric_f1\": str\n",
    "        評価関数名.\n",
    "    score: float\n",
    "        スコア.\n",
    "    True: bool\n",
    "        評価値が高い方が良いモデルか否か.\n",
    "        損失関数を使う場合はFalse.\n",
    "    \"\"\"\n",
    "    y_true = data.get_label()\n",
    "    preds = preds.reshape(params.NUM_CLASS, len(preds) // params.NUM_CLASS)\n",
    "    y_pred = np.argmax(preds, axis=0)\n",
    "    score = f1_score(y_true, y_pred, average=None) @ params.CLASS_WEIGHT\n",
    "    return \"metric_f1\", score, True\n",
    "\n",
    "\n",
    "def make_weight(x):\n",
    "    \"\"\"\n",
    "    Lightgbmのための重みづけ関数.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    x: int\n",
    "        ラベル番号.\n",
    "    \n",
    "    Returns:\n",
    "    -----------\n",
    "    params.CLASS_WEIGHT[x]: float\n",
    "        対応するラベルの重み.\n",
    "    \"\"\"\n",
    "    return params.CLASS_WEIGHT[x]\n",
    "\n",
    "\n",
    "def make_folded_df(csv_file, num_splits=4):\n",
    "    \"\"\"\n",
    "    fold番号を振るための関数.\n",
    "    StratifiedKFoldを使用するため、labelsという列名でラベルを保持する必要がある.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    csv_file: str\n",
    "        csvファイルのパス.\n",
    "    num_splits: int\n",
    "        フォールド数.\n",
    "    \n",
    "    Returns:\n",
    "    -----------\n",
    "    df: pd.DataFrame\n",
    "        foldにフォールド番号が入ったdf.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(csv_file)\n",
    "    \n",
    "    #==================================\n",
    "    df = df[df[\"jobflag\"] != 2].reset_index(drop=True)\n",
    "    re_labels = {1:1 , 3:2 , 4:3}\n",
    "    df[\"jobflag\"] = df[\"jobflag\"].map(re_labels)\n",
    "    #==================================\n",
    "    \n",
    "    df[params.TARGET] = df[params.TARGET] - 1\n",
    "    df[\"fold\"] = -1\n",
    "    df = df.rename(columns={params.TARGET: 'labels'})\n",
    "    label = df[\"labels\"].tolist()\n",
    "\n",
    "    skfold = StratifiedKFold(num_splits, shuffle=True, random_state=params.SEED)\n",
    "    for fold, (train_index, valid_index) in enumerate(skfold.split(range(len(label)), label)):\n",
    "        df['fold'].iloc[valid_index] = fold\n",
    "    return df\n",
    "\n",
    "\n",
    "def make_dataset(df, tokenizer, device, model_name):\n",
    "    \"\"\"\n",
    "    NLPモデル用のデータセットを作成するための関数.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    df: pd.DataFrame\n",
    "        モデル用のデータセット.\n",
    "    tokenizer: transformers.AutoTokenizer.from_pretrained\n",
    "        モデル用のtokenizer.\n",
    "    device: str\n",
    "        使用するデバイス. \"cpu\" or \"cuda\".\n",
    "    model_name: str\n",
    "        使用するモデルの名前.\n",
    "    \n",
    "    Returns:\n",
    "    ----------\n",
    "    dataset: nlp.Dataset.from_pandas\n",
    "        NLP用のデータセット.\n",
    "    \"\"\"\n",
    "    dataset = nlp.Dataset.from_pandas(df)\n",
    "    dataset = dataset.map(\n",
    "        lambda example: tokenizer(example[params.TEXT_COL],\n",
    "                                  padding=\"max_length\",\n",
    "                                  truncation=True,\n",
    "                                  max_length=params.MAX_TOKEN_LEN))\n",
    "    if not model_name in [\"roberta-base\", \"distilbert-base-uncased\"]:\n",
    "        dataset.set_format(type='torch', \n",
    "                           columns=['input_ids', 'token_type_ids', 'attention_mask', 'labels'], \n",
    "                           device=device)\n",
    "    else:\n",
    "        dataset.set_format(type='torch', \n",
    "                           columns=['input_ids', 'attention_mask', 'labels'], \n",
    "                           device=device)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def train_lgb(X, y, weight, n_folds=4):\n",
    "    \"\"\"\n",
    "    lightgbm用の訓練関数.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X: pd.DataFrame\n",
    "        訓練用の説明変数.\n",
    "    y: pd.DataFrame\n",
    "        訓練用の被説明変数.\n",
    "    weight: List[float]\n",
    "        訓練時のサンプルの重み.\n",
    "    n_folds: int\n",
    "        フォールド数.\n",
    "    \n",
    "    Returns:\n",
    "    -----------\n",
    "    scores: float\n",
    "        訓練時のOOFスコア.\n",
    "    feature_importances: pd.DataFrame\n",
    "        モデルの特徴量の重要度.\n",
    "    train_pred: np.array\n",
    "        訓練時のOOF予測値.\n",
    "    \"\"\"\n",
    "    train_pred = np.zeros((X.shape[0], y.nunique()), dtype='float32')\n",
    "    feature_importances = pd.DataFrame()\n",
    "    feature_importances['feature'] = X.columns\n",
    "    kfold = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=params.SEED)\n",
    "\n",
    "    print(\"LightGBM Training...\")\n",
    "    for fold, (train_idx, valid_idx) in enumerate(tqdm(kfold.split(X, y))):\n",
    "        X_train, X_valid = X.iloc[train_idx], X.iloc[valid_idx]\n",
    "        y_train, y_valid = y.iloc[train_idx], y.iloc[valid_idx]\n",
    "        weight_train, weight_valid = weight.iloc[train_idx], weight.iloc[valid_idx]\n",
    "        train_data = lgb.Dataset(X_train, label=y_train, weight=weight_train)\n",
    "        valid_data = lgb.Dataset(X_valid, label=y_valid, weight=weight_valid)\n",
    "        lgb_params = {\n",
    "            'objective': 'multiclass',\n",
    "            'num_class': 3,\n",
    "            'metric': 'None',\n",
    "            'learning_rate': 0.01,\n",
    "            'max_depth': -1,\n",
    "            'num_leaves': 31,\n",
    "            'max_bin': 31,\n",
    "            'min_data_in_leaf': 3,\n",
    "            'verbose': -1,\n",
    "            'seed': params.SEED,\n",
    "            'drop_seed': params.SEED,\n",
    "            'data_random_seed':params.SEED\n",
    "        }\n",
    "        model = lgb.train(lgb_params, train_data, valid_sets=[train_data,valid_data],\n",
    "                          num_boost_round=params.GBDT_ROUNDS,\n",
    "                          early_stopping_rounds=params.GBDT_EARLY_STOPPING,\n",
    "                          feval=metric_f1_lgb,\n",
    "                          verbose_eval=False, )\n",
    "        pickle.dump(model, open(params.MODELS_DIR+\"lgb_fold{}.lgbmodel\".format(fold),\n",
    "                                \"wb\"))\n",
    "        y_val_pred = model.predict(X_valid)\n",
    "        train_pred[valid_idx,:] = y_val_pred\n",
    "        feature_importances['fold_{}'.format(fold)] = model.feature_importance(importance_type='gain')\n",
    "        gc.collect()\n",
    "\n",
    "    feature_importances['importance'] = feature_importances.iloc[:,1:1+n_folds].mean(axis=1)\n",
    "    feature_importances = feature_importances.sort_values(by='importance', ascending=False)\n",
    "    scores = f1_score(y, np.argmax(train_pred, axis=1), average=None) @ params.CLASS_WEIGHT\n",
    "    return scores, feature_importances, train_pred\n",
    "\n",
    "\n",
    "def train_ctb(X, y, n_folds=4):\n",
    "    \"\"\"\n",
    "    catboost用の訓練関数.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X: pd.DataFrame\n",
    "        訓練用の説明変数.\n",
    "    y: pd.DataFrame\n",
    "        訓練用の被説明変数.\n",
    "    n_folds: int\n",
    "        フォールド数.\n",
    "    \n",
    "    Returns:\n",
    "    -----------\n",
    "    scores: float\n",
    "        訓練時のOOFスコア.\n",
    "    train_pred: np.array\n",
    "        訓練時のOOF予測値.\n",
    "    \"\"\"\n",
    "    train_pred = np.zeros((X.shape[0], y.nunique()), dtype='float32')\n",
    "    kfold = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=params.SEED)\n",
    "\n",
    "    print(\"CatBoost Training...\")\n",
    "    for fold, (train_idx, valid_idx) in enumerate(tqdm(kfold.split(X, y))):\n",
    "        X_train, X_valid = X.iloc[train_idx], X.iloc[valid_idx]\n",
    "        y_train, y_valid = y.iloc[train_idx], y.iloc[valid_idx]\n",
    "        train_data = Pool(X_train, label=y_train, text_features=[params.TEXT_COL])\n",
    "        valid_data = Pool(X_valid, label=y_valid, text_features=[params.TEXT_COL])\n",
    "        ctb_params = {\n",
    "            'objective': 'MultiClass',\n",
    "            'loss_function': 'TotalF1',\n",
    "            'class_weights': params.CLASS_WEIGHT.tolist(),\n",
    "            'num_boost_round':params.GBDT_ROUNDS,\n",
    "            'early_stopping_rounds':params.GBDT_EARLY_STOPPING,\n",
    "            'learning_rate':0.03,\n",
    "            'l2_leaf_reg':3.0,\n",
    "            #'subsample':0.66,\n",
    "            'max_depth':6,\n",
    "            'grow_policy':'SymmetricTree',\n",
    "            'min_data_in_leaf':1,\n",
    "            'max_leaves':31,\n",
    "            'verbose':False,\n",
    "            'random_seed':params.SEED,\n",
    "        }\n",
    "        model = CatBoost(ctb_params)\n",
    "        model.fit(train_data, eval_set=[valid_data], use_best_model=True, plot=False)\n",
    "        pickle.dump(model, open(params.MODELS_DIR+\"ctb_fold{}.ctbmodel\".format(fold),\n",
    "                                \"wb\"))\n",
    "        train_pred[valid_idx, :] = model.predict(X_valid)\n",
    "        gc.collect()\n",
    "    scores = f1_score(y, np.argmax(train_pred, axis=1), average=None) @ params.CLASS_WEIGHT\n",
    "    return scores, train_pred\n",
    "\n",
    "\n",
    "def train_fn(dataloader, model, criterion, optimizer, device, epoch):\n",
    "    \"\"\"\n",
    "    NLPモデル訓練EPOCH用関数.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    dataloader: torch.dataset.dataloader\n",
    "        NLP用のデータローダー.\n",
    "    model: torch.nn.Module\n",
    "        NLP用のtorchのモデル.\n",
    "    criterion: torch.nn.*Loss\n",
    "        NLP用の損失関数. 自分で作成した関数も可能.\n",
    "    optimizer: torch.optim.*\n",
    "        NLP用の最適化関数.\n",
    "    device: str\n",
    "        使用するデバイス. \"cuda\" or \"cpu\".\n",
    "    epoch: int\n",
    "        学習するエポック数.\n",
    "    \n",
    "    Returns:\n",
    "    ---------\n",
    "    train_losses: float\n",
    "        訓練時の累積損失.\n",
    "    train_acc: float\n",
    "        訓練時の正解率.\n",
    "    train_f1: float\n",
    "        訓練時のF1.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    train_losses = 0\n",
    "    correct_counts = 0\n",
    "    train_labels = []\n",
    "    train_preds = []\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        if len(batch.values())==4:\n",
    "            attention_mask, input_ids, labels, token_type_ids = batch.values()\n",
    "        else:\n",
    "            attention_mask, input_ids, labels = batch.values()\n",
    "            token_type_ids = None\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask, token_type_ids)\n",
    "        loss = criterion(outputs, labels)\n",
    "        _, preds = torch.max(outputs, axis=1)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_losses += loss.item()\n",
    "        correct_counts += torch.sum(preds == labels)\n",
    "\n",
    "        train_labels += labels.tolist()\n",
    "        train_preds += preds.tolist()\n",
    "\n",
    "    train_losses = train_losses / len(dataloader)\n",
    "    train_acc = correct_counts.double().cpu().detach().numpy() / len(dataloader.dataset)\n",
    "    train_f1 = metric_f1(train_labels, train_preds)\n",
    "\n",
    "    return train_losses, train_acc, train_f1\n",
    "\n",
    "\n",
    "def eval_fn(dataloader, model, criterion, device):\n",
    "    \"\"\"\n",
    "    NLPモデル検証EPOCH用関数.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    dataloader: torch.dataset.dataloader\n",
    "        NLP用のデータローダー.\n",
    "    model: torch.nn.Module\n",
    "        NLP用のtorchのモデル.\n",
    "    criterion: torch.nn.*Loss\n",
    "        NLP用の損失関数. 自分で作成した関数も可能.\n",
    "    device: str\n",
    "        使用するデバイス. \"cuda\" or \"cpu\".\n",
    "\n",
    "    Returns:\n",
    "    ---------\n",
    "    valid_losses: float\n",
    "        検証時の累積損失.\n",
    "    valid_acc: float\n",
    "        検証時の正解率.\n",
    "    valid_f1: float\n",
    "        検証時のF1.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    valid_losses = 0\n",
    "    total_corrects = 0\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(dataloader):\n",
    "            if len(batch.values())==4:\n",
    "                attention_mask, input_ids, labels, token_type_ids = batch.values()\n",
    "            else:\n",
    "                attention_mask, input_ids, labels = batch.values()\n",
    "                token_type_ids = None\n",
    "            outputs = model(input_ids, attention_mask, token_type_ids)\n",
    "            loss = criterion(outputs, labels)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            valid_losses += loss.item()\n",
    "            total_corrects += torch.sum(preds == labels)\n",
    "            all_labels += labels.tolist()\n",
    "            all_preds += preds.tolist()\n",
    "\n",
    "    valid_losses = valid_losses / len(dataloader)\n",
    "    valid_acc = total_corrects.double().cpu().detach().numpy() / len(dataloader.dataset)\n",
    "\n",
    "    valid_f1 = metric_f1(all_labels, all_preds)\n",
    "\n",
    "    return valid_losses, valid_acc, valid_f1\n",
    "\n",
    "\n",
    "def trainer(fold, df, model_name, oof_pred, typ):\n",
    "    \"\"\"\n",
    "    NLP訓練全Fold用関数.\n",
    "    F1で保存する.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    fold: int\n",
    "        検証に使用するフォールドの番号.\n",
    "    df: pd.DataFrame\n",
    "        学習に使用するデータフレーム.\n",
    "    model_name: str\n",
    "        NLPモデルの名前.\n",
    "    oof_pred: np.array\n",
    "        OOF予測値.\n",
    "    typ: str\n",
    "        NLPモデルから特徴量を取る位置.\n",
    "\n",
    "    Returns:\n",
    "    ----------\n",
    "    best_f1: float\n",
    "        保存したモデルのF1.\n",
    "    oof_pred: np.array\n",
    "        OOF予測値.\n",
    "    \"\"\"\n",
    "    train_df = df[df.fold != fold].reset_index(drop=True)\n",
    "    valid_df = df[df.fold == fold].reset_index(drop=True)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    train_dataset = make_dataset(train_df, tokenizer, params.DEVICE, model_name)\n",
    "    valid_dataset = make_dataset(valid_df, tokenizer, params.DEVICE, model_name)\n",
    "    train_dataloader = torch.utils.data.DataLoader(\n",
    "        train_dataset, batch_size=params.TRAIN_BATCH_SIZE, shuffle=True\n",
    "    )\n",
    "    valid_dataloader = torch.utils.data.DataLoader(\n",
    "        valid_dataset, batch_size=params.VALID_BATCH_SIZE, shuffle=False\n",
    "    )\n",
    "\n",
    "    model = Classifier(model_name, typ, num_classes=params.NUM_CLASS)\n",
    "    model = model.to(params.DEVICE)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss(weight=params.CLASS_WEIGHT_TENSOR.float())\n",
    "    optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "    train_losses = []\n",
    "    train_accs = []\n",
    "    train_f1s = []\n",
    "    valid_losses = []\n",
    "    valid_accs = []\n",
    "    valid_f1s = []\n",
    "\n",
    "    best_loss = np.inf\n",
    "    best_acc = 0\n",
    "    best_f1 = 0\n",
    "\n",
    "    for epoch in range(params.EPOCHS):\n",
    "        train_loss, train_acc, train_f1 = train_fn(train_dataloader, model, criterion, optimizer, params.DEVICE, epoch)\n",
    "        valid_loss, valid_acc, valid_f1 = eval_fn(valid_dataloader, model, criterion, params.DEVICE)\n",
    "        #print(f\"Loss: {valid_loss}  Acc: {valid_acc}  f1: {valid_f1}  \", end=\"\")\n",
    "\n",
    "        train_losses.append(train_loss)\n",
    "        train_accs.append(train_acc)\n",
    "        train_f1s.append(train_f1)\n",
    "        valid_losses.append(valid_loss)\n",
    "        valid_accs.append(valid_acc)\n",
    "        valid_f1s.append(valid_f1)\n",
    "\n",
    "        best_loss = valid_loss if valid_loss < best_loss else best_loss\n",
    "        besl_acc = valid_acc if valid_acc > best_acc else best_acc\n",
    "        if valid_f1 > best_f1:\n",
    "            best_f1 = valid_f1\n",
    "            #print(\"model saving!\", end=\"\")\n",
    "            torch.save(model.state_dict(), params.MODELS_DIR + f\"best_{model_name}_{typ}_{fold}.pth\")\n",
    "        #print(\"\\n\")\n",
    "\n",
    "    valid_pred = []\n",
    "    model.load_state_dict(torch.load(params.MODELS_DIR + f\"best_{model_name}_{typ}_{fold}.pth\"))\n",
    "    model.to(params.DEVICE)\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(valid_dataloader):\n",
    "            if len(batch.values())==4:\n",
    "                attention_mask, input_ids, labels, token_type_ids = batch.values()\n",
    "            else:\n",
    "                attention_mask, input_ids, labels = batch.values()\n",
    "                token_type_ids = None\n",
    "            outputs = model(input_ids, attention_mask, token_type_ids)\n",
    "            valid_pred += outputs.tolist()\n",
    "    oof_pred[df[df.fold == fold].index, :] = valid_pred\n",
    "    return best_f1, oof_pred\n",
    "\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    \"\"\"\n",
    "    NLPタスク分類用モデルクラス.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    model_name: str\n",
    "        使用するモデルの名前.\n",
    "    typ: str\n",
    "        NLPモデルから特徴量を取る位置.\n",
    "    num_classes: int\n",
    "        学習するデータのクラス数.\n",
    "    \"\"\"\n",
    "    def __init__(self, model_name, typ, num_classes=4):\n",
    "        super().__init__()\n",
    "\n",
    "        self.name = model_name\n",
    "        self.typ = typ\n",
    "        if model_name in [\"albert-large-v2\", \"xlm-mlm-ende-1024\"]:\n",
    "            nodes = 1024\n",
    "        else:\n",
    "            nodes = 768\n",
    "\n",
    "        self.bert = AutoModel.from_pretrained(model_name)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        if typ != \"ht\":\n",
    "            self.linear = nn.Linear(nodes, num_classes)\n",
    "        else:\n",
    "            self.linear = nn.Linear(nodes*2, num_classes)\n",
    "        nn.init.normal_(self.linear.weight, std=0.02)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids=None):\n",
    "        if self.name in [\"bert-base-uncased\", \"albert-base-v2\",\"jjzha/jobbert-base-cased\"]:\n",
    "            output, _ = self.bert(\n",
    "                input_ids = input_ids,\n",
    "                attention_mask = attention_mask,\n",
    "                token_type_ids = token_type_ids,\n",
    "                return_dict=False)\n",
    "            #output = output[:, 0, :]\n",
    "        elif self.name in [\"xlnet-base-cased\", \"xlm-mlm-ende-1024\"]:\n",
    "            output = self.bert(\n",
    "                input_ids = input_ids,\n",
    "                attention_mask = attention_mask,\n",
    "                token_type_ids = token_type_ids)\n",
    "            output = output[0]\n",
    "            #output = output[:, 0, :]\n",
    "        elif self.name in [\"roberta-base\", 'microsoft/deberta-base',\"microsoft/deberta-v3-base\",\"distilbert-base-uncased\"]:\n",
    "            output = self.bert(\n",
    "                input_ids = input_ids,\n",
    "                attention_mask = attention_mask,\n",
    "                 return_dict=False\n",
    "                )\n",
    "            output = output[0]\n",
    "            #output = output[:, 0, :]\n",
    "        \n",
    "        if self.typ == \"h\":\n",
    "            output = output[:, 0, :]\n",
    "        elif self.typ == \"m\":\n",
    "            output = torch.mean(output, dim=1)\n",
    "        elif self.typ == \"t\" or self.typ==\"FRt\" or self.typ==\"DEt\":\n",
    "            output = output[:, -1, :]\n",
    "        elif self.typ ==  \"ht\":\n",
    "            output = torch.cat((output[:, 0, :], output[:, -1, :]), dim=-1)\n",
    "        else:\n",
    "            output = output[:, 0, :]\n",
    "        output = self.dropout(output)\n",
    "        output = self.linear(output)\n",
    "        return output\n",
    "\n",
    "\n",
    "class Parameters(object):\n",
    "    \"\"\"\n",
    "    パラメータ管理用のクラス.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.SEED = 2020\n",
    "        # コードのパス. os.getcwd()が動かない場合はstrで直接渡す.\n",
    "        #BASE_PATH = \"C:/StudentCup2020/2nd/\"\n",
    "        self.BASE_PATH = os.getcwd() + '/'\n",
    "        self.TRAIN_FILE = self.BASE_PATH + \"data/train.csv\"\n",
    "        self.TEXT_COL = \"description\"\n",
    "        self.TARGET = \"jobflag\"\n",
    "        self.NUM_CLASS = 3\n",
    "        \n",
    "        self.LGB_TRAIN_FILE = self.BASE_PATH+\"data/lgb_train.csv\"\n",
    "        self.OUTPUT_PATH = self.BASE_PATH + \"outputs/\"\n",
    "        \n",
    "        \n",
    "        self.TRAIN_WEIGHT = np.array([0.3,0.3,0.35])\n",
    "        self.TEST_WEIGHT = np.array([0.3,0.3,0.35])\n",
    "\n",
    "        self.CLASS_WEIGHT = self.TEST_WEIGHT / self.TRAIN_WEIGHT\n",
    "        self.CLASS_WEIGHT /= sum(self.CLASS_WEIGHT)\n",
    "        self.CLASS_WEIGHT_TENSOR = torch.tensor(self.CLASS_WEIGHT).cuda()\n",
    "        \n",
    "        self.DEVICE = \"cuda\"\n",
    "        self.MODELS_DIR = self.BASE_PATH + \"models/\"\n",
    "        self.EPOCHS = 5\n",
    "        self.GBDT_ROUNDS = 2000\n",
    "        self.GBDT_EARLY_STOPPING = 100\n",
    "        self.NUM_SPLITS = 4\n",
    "        \n",
    "        self.TRAIN_BATCH_SIZE = 32\n",
    "        self.VALID_BATCH_SIZE = 128\n",
    "        self.MAX_TOKEN_LEN = 128\n",
    "params = Parameters()\n",
    "\n",
    "\n",
    "def main(params):\n",
    "    \"\"\"\n",
    "    \n",
    "    # --- lightgbm --- #\n",
    "    lgb_df = pd.read_csv(params.LGB_TRAIN_FILE)\n",
    "    \n",
    "    lgb_df = lgb_df[lgb_df[\"jobflag\"] != 2].reset_index(drop=True)\n",
    "    re_labels = {1:1 , 3:2 , 4:3}\n",
    "    lgb_df[\"jobflag\"] = lgb_df[\"jobflag\"].map(re_labels)\n",
    "    \n",
    "    X = lgb_df.drop([params.TARGET], axis=1)\n",
    "    y = lgb_df[params.TARGET] - 1\n",
    "    weight = y.apply(lambda x: make_weight(x))\n",
    "    scores, feature_importances, train_pred = train_lgb(X, y, weight, n_folds=params.NUM_SPLITS)\n",
    "    print(\"LightGBM Score: {}\".format(scores))\n",
    "    feature_importances.to_csv(params.OUTPUT_PATH+\"lgb_feature_importances.csv\", index=False)\n",
    "    np.save(params.OUTPUT_PATH+\"lgb_trap\", train_pred)\n",
    "\n",
    "    # --- catboost --- #\n",
    "    train = pd.read_csv(params.TRAIN_FILE).drop(['id'], axis=1)\n",
    "    \n",
    "    train = train[train[\"jobflag\"] != 2].reset_index(drop=True)\n",
    "    re_labels = {1:1 , 3:2 , 4:3}\n",
    "    train[\"jobflag\"] = train[\"jobflag\"].map(re_labels)\n",
    "    \n",
    "    train[params.TARGET] -= 1\n",
    "    col = [c for c in train.columns if c not in ['id', params.TARGET]]\n",
    "    X = train[col]\n",
    "    y = train[params.TARGET].astype(int)\n",
    "    scores, train_pred = train_ctb(X, y, n_folds=params.NUM_SPLITS)\n",
    "    print(\"CatBoost Score: {}\".format(scores))\n",
    "    np.save(params.OUTPUT_PATH+\"cat_trap\", train_pred)\n",
    "    \n",
    "    # --- roberta --- #\n",
    "    print(\"roberta Training...\")\n",
    "    df = make_folded_df(params.TRAIN_FILE, params.NUM_SPLITS)\n",
    "    model_name = \"roberta-base\"\n",
    "    typs = [\"h\", \"m\", \"t\", \"ht\"]\n",
    "    for typ in typs:\n",
    "        f1_scores = []\n",
    "        oof_pred = np.zeros((len(df), params.NUM_CLASS), dtype='float32')\n",
    "        print(\"=\"*10 + \"roberta {} Training\".format(typ) + \"=\"*10)\n",
    "        for fold in tqdm(range(params.NUM_SPLITS)):\n",
    "            f1, oof_pred = trainer(fold, df, model_name, oof_pred, typ)\n",
    "            f1_scores.append(f1)\n",
    "        scores = metric_f1(df['labels'], np.argmax(oof_pred, axis=1))\n",
    "        print(\"roberta {} Score: {}\".format(typ, scores))\n",
    "        np.save(params.OUTPUT_PATH+model_name+\"_\"+typ+\"_trap\", oof_pred)\n",
    "    \n",
    "    # --- deberta --- #\n",
    "    print(\"deberta Training...\")\n",
    "    df = make_folded_df(params.TRAIN_FILE, params.NUM_SPLITS)\n",
    "    model_name = \"microsoft/deberta-base\"\n",
    "    typs = [\"h\", \"m\", \"t\", \"ht\"]\n",
    "    for typ in typs:\n",
    "        f1_scores = []\n",
    "        oof_pred = np.zeros((len(df), params.NUM_CLASS), dtype='float32')\n",
    "        print(\"=\"*10 + \"deberta {} Training\".format(typ) + \"=\"*10)\n",
    "        for fold in tqdm(range(params.NUM_SPLITS)):\n",
    "            f1, oof_pred = trainer(fold, df, model_name, oof_pred, typ)\n",
    "            f1_scores.append(f1)\n",
    "        scores = metric_f1(df['labels'], np.argmax(oof_pred, axis=1))\n",
    "        print(\"deberta {} Score: {}\".format(typ, scores))\n",
    "        np.save(params.OUTPUT_PATH+model_name+\"_\"+typ+\"_trap\", oof_pred)\n",
    "        \n",
    "    # --- bert --- #\n",
    "    print(\"bert Training...\")\n",
    "    df = make_folded_df(params.TRAIN_FILE, params.NUM_SPLITS)\n",
    "    model_name = \"bert-base-uncased\"\n",
    "    typs = [\"h\", \"m\", \"t\", \"ht\"]\n",
    "    for typ in typs:\n",
    "        f1_scores = []\n",
    "        oof_pred = np.zeros((len(df), params.NUM_CLASS), dtype='float32')\n",
    "        print(\"=\"*10 + \"bert {} Training\".format(typ) + \"=\"*10)\n",
    "        for fold in tqdm(range(params.NUM_SPLITS)):\n",
    "            f1, oof_pred = trainer(fold, df, model_name, oof_pred, typ)\n",
    "            f1_scores.append(f1)\n",
    "        scores = metric_f1(df['labels'], np.argmax(oof_pred, axis=1))\n",
    "        print(\"bert {} Score: {}\".format(typ, scores))\n",
    "        np.save(params.OUTPUT_PATH+model_name+\"_\"+typ+\"_trap\", oof_pred)\n",
    "\n",
    "    \n",
    "    # --- xlnet --- #\n",
    "    print(\"xlnet Training...\")\n",
    "    df = make_folded_df(params.TRAIN_FILE, params.NUM_SPLITS)\n",
    "    model_name = \"xlnet-base-cased\"\n",
    "    typs = [\"h\", \"m\", \"t\", \"ht\"]\n",
    "    for typ in typs:\n",
    "        f1_scores = []\n",
    "        oof_pred = np.zeros((len(df), params.NUM_CLASS), dtype='float32')\n",
    "        print(\"=\"*10 + \"xlnet {} Training\".format(typ) + \"=\"*10)\n",
    "        for fold in tqdm(range(params.NUM_SPLITS)):\n",
    "            f1, oof_pred = trainer(fold, df, model_name, oof_pred, typ)\n",
    "            f1_scores.append(f1)\n",
    "        scores = metric_f1(df['labels'], np.argmax(oof_pred, axis=1))\n",
    "        print(\"xlnet {} Score: {}\".format(typ, scores))\n",
    "        np.save(params.OUTPUT_PATH+model_name+\"_\"+typ+\"_trap\", oof_pred)\n",
    "        \n",
    "    # --- deberta-v3 --- #\n",
    "    print(\"deberta-v3 Training...\")\n",
    "    df = make_folded_df(params.TRAIN_FILE, params.NUM_SPLITS)\n",
    "    model_name = \"microsoft/deberta-v3-base\"\n",
    "    typs = [\"h\", \"m\", \"t\", \"ht\"]\n",
    "    for typ in typs:\n",
    "        f1_scores = []\n",
    "        oof_pred = np.zeros((len(df), params.NUM_CLASS), dtype='float32')\n",
    "        print(\"=\"*10 + \"deberta-v3 {} Training\".format(typ) + \"=\"*10)\n",
    "        for fold in tqdm(range(params.NUM_SPLITS)):\n",
    "            f1, oof_pred = trainer(fold, df, model_name, oof_pred, typ)\n",
    "            f1_scores.append(f1)\n",
    "        scores = metric_f1(df['labels'], np.argmax(oof_pred, axis=1))\n",
    "        print(\"deberta-v3 {} Score: {}\".format(typ, scores))\n",
    "        np.save(params.OUTPUT_PATH+model_name+\"_\"+typ+\"_trap\", oof_pred)\n",
    "\n",
    "    # --- distilbert --- #\n",
    "    print(\"distilbert Training...\")\n",
    "    df = make_folded_df(params.TRAIN_FILE, params.NUM_SPLITS)\n",
    "    model_name = \"distilbert-base-uncased\"\n",
    "    typs = [\"h\", \"m\", \"t\", \"ht\"]\n",
    "    for typ in typs:\n",
    "        f1_scores = []\n",
    "        oof_pred = np.zeros((len(df), params.NUM_CLASS), dtype='float32')\n",
    "        print(\"=\"*10 + \"distilbert {} Training\".format(typ) + \"=\"*10)\n",
    "        for fold in tqdm(range(params.NUM_SPLITS)):\n",
    "            f1, oof_pred = trainer(fold, df, model_name, oof_pred, typ)\n",
    "            f1_scores.append(f1)\n",
    "        scores = metric_f1(df['labels'], np.argmax(oof_pred, axis=1))\n",
    "        print(\"distilbert {} Score: {}\".format(typ, scores))\n",
    "        np.save(params.OUTPUT_PATH+model_name+\"_\"+typ+\"_trap\", oof_pred)\n",
    "    \n",
    "    # --- albert --- #\n",
    "    print(\"albert Training...\")\n",
    "    df = make_folded_df(params.TRAIN_FILE, params.NUM_SPLITS)\n",
    "    model_name = \"albert-base-v2\"\n",
    "    typs = [\"h\", \"m\", \"t\", \"ht\"]\n",
    "    for typ in typs:\n",
    "        f1_scores = []\n",
    "        oof_pred = np.zeros((len(df), params.NUM_CLASS), dtype='float32')\n",
    "        print(\"=\"*10 + \"albert {} Training\".format(typ) + \"=\"*10)\n",
    "        for fold in tqdm(range(params.NUM_SPLITS)):\n",
    "            f1, oof_pred = trainer(fold, df, model_name, oof_pred, typ)\n",
    "            f1_scores.append(f1)\n",
    "        scores = metric_f1(df['labels'], np.argmax(oof_pred, axis=1))\n",
    "        print(\"albert {} Score: {}\".format(typ, scores))\n",
    "        np.save(params.OUTPUT_PATH+model_name+\"_\"+typ+\"_trap\", oof_pred)\n",
    "    \n",
    "    # --- jobbert --- #\n",
    "    print(\"jobbert Training...\")\n",
    "    df = make_folded_df(params.TRAIN_FILE, params.NUM_SPLITS)\n",
    "    model_name = \"jjzha/jobbert-base-cased\"\n",
    "    typs = [\"h\", \"m\", \"t\", \"ht\"]\n",
    "    for typ in typs:\n",
    "        f1_scores = []\n",
    "        oof_pred = np.zeros((len(df), params.NUM_CLASS), dtype='float32')\n",
    "        print(\"=\"*10 + \"jobbert {} Training\".format(typ) + \"=\"*10)\n",
    "        for fold in tqdm(range(params.NUM_SPLITS)):\n",
    "            f1, oof_pred = trainer(fold, df, model_name, oof_pred, typ)\n",
    "            f1_scores.append(f1)\n",
    "        scores = metric_f1(df['labels'], np.argmax(oof_pred, axis=1))\n",
    "        print(\"jobbert {} Score: {}\".format(typ, scores))\n",
    "        np.save(params.OUTPUT_PATH+model_name+\"_\"+typ+\"_trap\", oof_pred)\n",
    "      \n",
    "    \"\"\"\n",
    "    # --- ensemble --- #\n",
    "    print(\"Ensemble...\")\n",
    "    model_names = [\"lgb\",\n",
    "                   \"cat\",\n",
    "                    \"jjzha/jobbert-base-cased_h\",\n",
    "                   \"jjzha/jobbert-base-cased_m\",\n",
    "                   \"jjzha/jobbert-base-cased_t\",\n",
    "                   \"jjzha/jobbert-base-cased_ht\",\n",
    "                    \"albert-base-v2_h\",\n",
    "                   \"albert-base-v2_m\",\n",
    "                  \"albert-base-v2_t\",\n",
    "                   \"albert-base-v2_ht\",\n",
    "                   \"roberta-base_h\",\n",
    "                   \"roberta-base_m\",\n",
    "                   \"roberta-base_t\",\n",
    "                   \"roberta-base_ht\",\n",
    "                   \"distilbert-base-uncased_h\",\n",
    "                   \"distilbert-base-uncased_m\",\n",
    "                   \"distilbert-base-uncased_t\",\n",
    "                   \"distilbert-base-uncased_ht\",\n",
    "                   \"xlnet-base-cased_h\",\n",
    "                   \"xlnet-base-cased_m\",\n",
    "                   \"xlnet-base-cased_t\",\n",
    "                   \"xlnet-base-cased_ht\",\n",
    "                   \"bert-base-uncased_h\",\n",
    "                   \"bert-base-uncased_m\",\n",
    "                   \"bert-base-uncased_t\",\n",
    "                   \"bert-base-uncased_ht\",\n",
    "                   \"microsoft/deberta-base_h\",\n",
    "                   \"microsoft/deberta-base_m\", \n",
    "                   \"microsoft/deberta-base_t\",\n",
    "                   \"microsoft/deberta-base_ht\",\n",
    "                  \"microsoft/deberta-v3-base_h\",\n",
    "                   \"microsoft/deberta-v3-base_m\", \n",
    "                   \"microsoft/deberta-v3-base_t\",\n",
    "                   \"microsoft/deberta-v3-base_ht\"]\n",
    "    train = pd.read_csv(params.TRAIN_FILE)\n",
    "    \n",
    "    train = train[train[\"jobflag\"] != 2].reset_index(drop=True)\n",
    "    re_labels = {1:1 , 3:2 , 4:3}\n",
    "    train[\"jobflag\"] = train[\"jobflag\"].map(re_labels)\n",
    "    \n",
    "    \n",
    "    train[\"label\"] = train[params.TARGET] - 1\n",
    "    train_pred = np.zeros((train.shape[0], 3, len(model_names)))\n",
    "    for i, model_name in enumerate(model_names):\n",
    "        trap = np.load(params.OUTPUT_PATH+model_name+\"_trap.npy\")\n",
    "        train_pred[:, :, i] = trap\n",
    "\n",
    "    best_w = np.ones(len(model_names))\n",
    "    best_w /= sum(best_w)\n",
    "    trap = np.average(train_pred, axis=2, weights=best_w)\n",
    "    best_cw = 0.5 + np.ones(3)\n",
    "    best_cw /= sum(best_cw)\n",
    "    trap *= best_cw\n",
    "    best_score = f1_score(train['label'], np.argmax(trap, axis=1), average=None) @ params.CLASS_WEIGHT\n",
    "    for i in tqdm(range(100_000)): # 100_000\n",
    "        w = np.random.random(len(model_names))\n",
    "        w /= sum(w)\n",
    "        trap = np.average(train_pred, axis=2, weights=w)\n",
    "        cw = 0.5 + np.random.random(3)\n",
    "        cw /= sum(cw)\n",
    "        trap = trap * cw\n",
    "        score = f1_score(train['label'], np.argmax(trap, axis=1), average=None) @ params.CLASS_WEIGHT\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_w = w\n",
    "            best_cw = cw\n",
    "    print(\"Best Ensemble Score: {}\".format(best_score))\n",
    "    oof_pred = np.average(train_pred, axis=2, weights=best_w)\n",
    "    oof_pred = oof_pred * best_cw\n",
    "    np.save(params.OUTPUT_PATH+\"trap_ensemble\", oof_pred)\n",
    "    np.save(params.OUTPUT_PATH+\"config_ensemble_bestw\", best_w)\n",
    "    np.save(params.OUTPUT_PATH+\"config_ensemble_bestcw\", best_cw)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    seed_everything(params.SEED)\n",
    "    if \"models\" not in os.listdir(params.BASE_PATH):\n",
    "        os.mkdir(params.BASE_PATH + \"models/\")\n",
    "    if \"outputs\" not in os.listdir(params.BASE_PATH):\n",
    "        os.mkdir(params.BASE_PATH + \"outputs/\")\n",
    "    main(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141b94ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "0.8393948614719628"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6da4e57",
   "metadata": {},
   "source": [
    "- Best Ensemble Score: 0.7502827367369933"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b89c39",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
