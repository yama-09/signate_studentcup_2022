{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ddea38a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: NVIDIA GeForce RTX 3060\n",
      "fold 0 ================================================================================\n",
      "roberta-base\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20f2d0c106664ae4824326897cde94b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1494 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84c63d05c1c44ec08af3cc09ebffcabe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/374 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b31cf9440233436fa2185d907ba69eb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/47 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dee5b5ae09104b4296dbc6adf488aa06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.662127157052358  Acc: 0.7379679144385026  f1: 0.7404431329622629  model saving!\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8eaa1ad73d90405b8990b1181c0923e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/47 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34a1a60010b44aa19fb497f5346f2344",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.5889774163564047  Acc: 0.7914438502673797  f1: 0.7927133567868517  model saving!\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78c5b2a2fdcb41bab9c858a43dd9379c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/47 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46a57cdd62154430ab8086f9bcde1aae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.6268282532691956  Acc: 0.7566844919786097  f1: 0.7603440055865389  \n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1ed8209631a443ea9cdd591717ae35c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/47 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7a997ff569f448789709baff93f21c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.6634980936845144  Acc: 0.7941176470588235  f1: 0.7969724296674764  model saving!\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b146a81e8e247f29bdded552fa0c69c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/47 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6997aadd2a0496d864b2b90a7dc82e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.7176517446835836  Acc: 0.7941176470588235  f1: 0.7944020698981473  \n",
      "\n",
      "<fold=0> best score: 0.7969724296674764\n",
      "\n",
      "fold 1 ================================================================================\n",
      "roberta-base\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "274c0786a63b4f4f80ba60216f4f6788",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1494 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "360b66eff5ce44aebb1f876774e866c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/374 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1902529ae0de4483be0a4ffbc3a4eac8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/47 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "462b1454749d4109bfb3d0b5ab4121ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.6576821406682333  Acc: 0.732620320855615  f1: 0.7329066347991471  model saving!\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "851c9f67f4ec41ad9b3f22515951378a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/47 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4d684421e6a4e6c91c03cd17b1e4e09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.6090842882792155  Acc: 0.7807486631016043  f1: 0.7823389417424762  model saving!\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "123149b3408249799e1485473b9f9c27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/47 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87850e1991f74cfab0a8ba0d8122ff25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.5749504367510477  Acc: 0.786096256684492  f1: 0.7827229180464114  model saving!\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78f535a3550147438f91cd81829d4f74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/47 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cace7a7127d444eb11ceae0a7e28d19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.6230521996816  Acc: 0.786096256684492  f1: 0.777870670995671  \n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a322f8378de14516b3b4e0fbf7c66d3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/47 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c26d3148c47040f0830db4f13121a29f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.6254672408103943  Acc: 0.8101604278074866  f1: 0.8136709920755029  model saving!\n",
      "\n",
      "<fold=1> best score: 0.8136709920755029\n",
      "\n",
      "fold 2 ================================================================================\n",
      "roberta-base\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d2321c859794317a33714e93f5e0778",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1494 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fefad18f52f4fe99dc36572b1011c5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/374 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5a3597b7bff47959bf907309be4fe45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/47 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d53a7ca8af104ce2863291b841545e79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.7605239947636923  Acc: 0.6844919786096256  f1: 0.6642931496847604  model saving!\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed2ed30696a148118aa30d1bf367ad3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/47 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88ae68d2d7e04071b27bd443f01c3943",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.5700366397698721  Acc: 0.7834224598930482  f1: 0.7839085800617365  model saving!\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87136109dfc546b2845a1d6a528813d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/47 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "646e93f78a0040a0893ae32466897c04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.48570077617963153  Acc: 0.8342245989304813  f1: 0.8328785183199864  model saving!\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19dc82397d2046139ffee1b29036b914",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/47 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58fd6906a6704c2ca80062a0c1cb5448",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.5518049796422323  Acc: 0.8074866310160428  f1: 0.8122874993800631  \n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46142298bf534a4c881a92835c68c832",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/47 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35afad320aea4f4a8cc07bf470ea7864",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.5264763236045837  Acc: 0.8368983957219251  f1: 0.8342198412147555  model saving!\n",
      "\n",
      "<fold=2> best score: 0.8342198412147555\n",
      "\n",
      "fold 3 ================================================================================\n",
      "roberta-base\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10fb28a63e9343f8bd1d9bf546be392a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1495 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1714c7b178d341be97fc15ef5b61d7d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/373 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95bb38fd2d24404c80b202b2e34ac2d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/47 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a752d703b7b747958812e21d775fab8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.7301376859347025  Acc: 0.6997319034852547  f1: 0.6916003555480006  model saving!\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1829ab38636483b82916ef5ef41f367",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/47 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cd47419792846de8785dc7acda708e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.5900732179482778  Acc: 0.7506702412868632  f1: 0.7549289937204303  model saving!\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31e199d62fde4b3684d711c1855f09b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/47 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "345e3cbe425d4d4abc39809027fa6d5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.4851677318414052  Acc: 0.8150134048257373  f1: 0.8196584841992149  model saving!\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "115ca5a29ee54e378c8d4c071d1e39ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/47 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57b9da9e71784c499085ae50dde8d5e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.4666588008403778  Acc: 0.8310991957104558  f1: 0.8326428055919382  model saving!\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3646f1b3cf242d4be66bed6d2a3430e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/47 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82a4381863bf4957b154cdfe9357b219",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.512673020362854  Acc: 0.8176943699731903  f1: 0.8207318161668683  \n",
      "\n",
      "<fold=3> best score: 0.8326428055919382\n",
      "\n",
      "fold 4 ================================================================================\n",
      "roberta-base\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47ab395d9fc4466e8ef547f7bd3efe57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1495 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "400abad45ff945f1abfb33b19d9cd315",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/373 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae368f778f424ee18fe713c5bce151af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/47 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b1bff016c794b37aadf7eb8c72cb09d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.745769739151001  Acc: 0.6943699731903485  f1: 0.6878397197770295  model saving!\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "116a352745144c1b98154988633620e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/47 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b52a06f40944c298ebb4929ed053f0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.5127642452716827  Acc: 0.7935656836461126  f1: 0.7945856099143866  model saving!\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "620c1701b4f144b8968e6d42bb44d107",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/47 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78f2b7240d8942e6b34acdf2aa71d3c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.4974621931711833  Acc: 0.8096514745308311  f1: 0.8089295739050822  model saving!\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c15db9d0e4745ea871773f19a0071fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/47 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72431380215b4e5cb2475b76da86ff81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.5501356919606527  Acc: 0.8016085790884718  f1: 0.8055775491600001  \n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7e882a0ae6c448da34e1cb8fa8e7613",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/47 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bccd4f67086a40c5b40659aa6c53f2ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.632986972729365  Acc: 0.8150134048257373  f1: 0.8103443254573697  model saving!\n",
      "\n",
      "<fold=4> best score: 0.8103443254573697\n",
      "\n",
      "CV: 0.8175700788014083\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c7d305a2cf748538c295da632f53baa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1517 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d91bb1b736eb4f099845e6370b6648fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import collections\n",
    "import os\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import nlp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import AdamW, AutoModel, AutoTokenizer\n",
    "import texthero as hero\n",
    "import re\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "# seeds\n",
    "SEED = 42\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "seed_everything(SEED)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    current_device = torch.cuda.current_device()\n",
    "    print(\"Device:\", torch.cuda.get_device_name(current_device))\n",
    "\n",
    "\n",
    "# config\n",
    "data_dir = os.path.join( \"input/\")\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "TRAIN_FILE = os.path.join(data_dir, \"train_Argu.csv\")\n",
    "TEST_FILE = os.path.join(data_dir, \"test_Argu.csv\")\n",
    "MODELS_DIR = \"models/\"\n",
    "MODEL_NAME = 'roberta-base' # roberta-base\n",
    "TRAIN_BATCH_SIZE = 32\n",
    "VALID_BATCH_SIZE = 128\n",
    "NUM_CLASSES = 4\n",
    "EPOCHS = 5\n",
    "NUM_SPLITS = 5\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# dataset\n",
    "def make_folded_df(csv_file, num_splits=5):\n",
    "    df = pd.read_csv(csv_file)\n",
    "    # ===================\n",
    "    #p = re.compile(r\"<[^>]*?>|&amp;|[.,/'’\\\"”]\")\n",
    "    #df['description'] = df['description'].map(lambda x: p.sub(\"\", x))\n",
    "    #df['description'] = df['description'].map(lambda x: x.lstrip())\n",
    "    # ===================\n",
    "    #df = df[df[\"jobflag\"] != 2].reset_index(drop=True)\n",
    "    #re_labels = {1:1 , 3:2 , 4:3}\n",
    "    #df[\"jobflag\"] = df[\"jobflag\"].map(re_labels)\n",
    "    # ===================\n",
    "    \n",
    "    df[\"jobflag\"] = df[\"jobflag\"] - 1\n",
    "    df[\"kfold\"] = np.nan\n",
    "    df = df.rename(columns={'jobflag': 'labels'})\n",
    "    label = df[\"labels\"].tolist()\n",
    "\n",
    "    skfold = StratifiedKFold(num_splits, shuffle=True, random_state=SEED)\n",
    "    for fold, (_, valid_indexes) in enumerate(skfold.split(range(len(label)), label)):\n",
    "        for i in valid_indexes:\n",
    "            df.iat[i,3] = fold\n",
    "    return df\n",
    "\n",
    "def make_dataset(df, tokenizer, device):\n",
    "    dataset = nlp.Dataset.from_pandas(df)\n",
    "    dataset = dataset.map(\n",
    "        lambda example: tokenizer(example[\"description\"],\n",
    "                                  padding=\"max_length\",\n",
    "                                  truncation=True,\n",
    "                                  max_length=128))\n",
    "    dataset.set_format(type='torch', \n",
    "                       columns=['input_ids', 'attention_mask', 'labels'], \n",
    "                       device=device)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "# model\n",
    "\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, model_name, num_classes=4):\n",
    "        super().__init__()\n",
    "\n",
    "        self.bert = AutoModel.from_pretrained(model_name)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.linear = nn.Linear(768, num_classes)\n",
    "        nn.init.normal_(self.linear.weight, std=0.02)\n",
    "        nn.init.zeros_(self.linear.bias)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        output = self.bert(\n",
    "                input_ids = input_ids,\n",
    "                attention_mask = attention_mask,\n",
    "                return_dict=False\n",
    "                )\n",
    "        output = output[0]\n",
    "        output = output[:, 0, :]\n",
    "        output = self.dropout(output)\n",
    "        output = self.linear(output)\n",
    "        \n",
    "        #print(output.shape)\n",
    "        #print(output[:, 0, :].shape)\n",
    "        return output\n",
    "\n",
    "\n",
    "# training function\n",
    "def train_fn(dataloader, model, criterion, optimizer, scheduler, device, epoch):\n",
    "    \n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_corrects = 0\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "\n",
    "    progress = tqdm(dataloader, total=len(dataloader))\n",
    "\n",
    "    for i, batch in enumerate(progress):\n",
    "        progress.set_description(f\"<Train> Epoch{epoch+1}\")\n",
    "\n",
    "        if len(batch.values())==4:\n",
    "            attention_mask, input_ids, labels, token_type_ids = batch.values()\n",
    "        else:\n",
    "            attention_mask, input_ids, labels = batch.values()\n",
    "            token_type_ids = None\n",
    "        del batch\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(input_ids, attention_mask, token_type_ids)\n",
    "        del input_ids, attention_mask, token_type_ids\n",
    "        \n",
    "        #print(labels.shape)\n",
    "        #outputs = outputs[0]\n",
    "        \n",
    "        loss = criterion(outputs, labels)  # 損失を計算\n",
    "        \n",
    "        _, preds = torch.max(outputs, 1)  # ラベルを予測\n",
    "        del outputs\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        del loss\n",
    "        total_corrects += torch.sum(preds == labels)\n",
    "\n",
    "        all_labels += labels.tolist()\n",
    "        all_preds += preds.tolist()\n",
    "        del labels, preds\n",
    "\n",
    "        progress.set_postfix(loss=total_loss/(i+1), f1=f1_score(all_labels, all_preds, average=\"macro\"))\n",
    "\n",
    "    train_loss = total_loss / len(dataloader)\n",
    "    train_acc = total_corrects.double().cpu().detach().numpy() / len(dataloader.dataset)\n",
    "    train_f1 = f1_score(all_labels, all_preds, average=\"macro\")\n",
    "\n",
    "    return train_loss, train_acc, train_f1\n",
    "\n",
    "\n",
    "def eval_fn(dataloader, model, criterion, device, epoch):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_corrects = 0\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        progress = tqdm(dataloader, total=len(dataloader))\n",
    "        \n",
    "        for i, batch in enumerate(progress):\n",
    "            progress.set_description(f\"<Valid> Epoch{epoch+1}\")\n",
    "\n",
    "            if len(batch.values())==4:\n",
    "                attention_mask, input_ids, labels, token_type_ids = batch.values()\n",
    "            else:\n",
    "                attention_mask, input_ids, labels = batch.values()\n",
    "                token_type_ids = None\n",
    "            del batch\n",
    "\n",
    "            outputs = model(input_ids, attention_mask, token_type_ids)\n",
    "            del input_ids, attention_mask, token_type_ids\n",
    "            \n",
    "            #outputs = outputs[0]\n",
    "            \n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            del outputs\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            del loss\n",
    "            total_corrects += torch.sum(preds == labels)\n",
    "\n",
    "            all_labels += labels.tolist()\n",
    "            all_preds += preds.tolist()\n",
    "            del labels, preds\n",
    "\n",
    "            progress.set_postfix(loss=total_loss/(i+1), f1=f1_score(all_labels, all_preds, average=\"macro\"))\n",
    "\n",
    "    valid_loss = total_loss / len(dataloader)\n",
    "    valid_acc = total_corrects.double().cpu().detach().numpy() / len(dataloader.dataset)\n",
    "\n",
    "    valid_f1 = f1_score(all_labels, all_preds, average=\"macro\")\n",
    "\n",
    "    return valid_loss, valid_acc, valid_f1\n",
    "\n",
    "\n",
    "def plot_training(train_losses, train_accs, train_f1s,\n",
    "                  valid_losses, valid_accs, valid_f1s,\n",
    "                  epoch, fold):\n",
    "    \n",
    "    loss_df = pd.DataFrame({\"Train\":train_losses,\n",
    "                            \"Valid\":valid_losses},\n",
    "                        index=range(1, epoch+2))\n",
    "    loss_ax = sns.lineplot(data=loss_df).get_figure()\n",
    "    loss_ax.savefig(f\"figures/loss_plot_fold={fold}.png\", dpi=300)\n",
    "    loss_ax.clf()\n",
    "\n",
    "    acc_df = pd.DataFrame({\"Train\":train_accs,\n",
    "                           \"Valid\":valid_accs},\n",
    "                          index=range(1, epoch+2))\n",
    "    acc_ax = sns.lineplot(data=acc_df).get_figure()\n",
    "    acc_ax.savefig(f\"figures/acc_plot_fold={fold}.png\", dpi=300)\n",
    "    acc_ax.clf()\n",
    "\n",
    "    f1_df = pd.DataFrame({\"Train\":train_f1s,\n",
    "                          \"Valid\":valid_f1s},\n",
    "                         index=range(1, epoch+2))\n",
    "    f1_ax = sns.lineplot(data=f1_df).get_figure()\n",
    "    f1_ax.savefig(f\"figures/f1_plot_fold={fold}.png\", dpi=300)\n",
    "    f1_ax.clf()\n",
    "\n",
    "def trainer(fold, df):\n",
    "    \n",
    "    train_df = df[df.kfold != fold].reset_index(drop=True)\n",
    "    valid_df = df[df.kfold == fold].reset_index(drop=True)\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "    \n",
    "    print(MODEL_NAME)\n",
    "    \n",
    "    train_dataset = make_dataset(train_df, tokenizer, DEVICE)\n",
    "    valid_dataset = make_dataset(valid_df, tokenizer, DEVICE)\n",
    "\n",
    "    train_dataloader = torch.utils.data.DataLoader(\n",
    "        train_dataset, batch_size=TRAIN_BATCH_SIZE, shuffle=True\n",
    "    )\n",
    "    valid_dataloader = torch.utils.data.DataLoader(\n",
    "        valid_dataset, batch_size=VALID_BATCH_SIZE, shuffle=False\n",
    "    )\n",
    "\n",
    "    model = Classifier(MODEL_NAME, num_classes=NUM_CLASSES)\n",
    "    model = model.to(DEVICE)\n",
    "    \n",
    "    \n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=100000, gamma=1.0)\n",
    "    # ダミーのスケジューラー\n",
    "\n",
    "    train_losses = []\n",
    "    train_accs = []\n",
    "    train_f1s = []\n",
    "    valid_losses = []\n",
    "    valid_accs = []\n",
    "    valid_f1s = []\n",
    "\n",
    "    best_loss = np.inf\n",
    "    best_acc = 0\n",
    "    best_f1 = 0\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        train_loss, train_acc, train_f1 = train_fn(train_dataloader, model, criterion, optimizer, scheduler, DEVICE, epoch)\n",
    "        valid_loss, valid_acc, valid_f1 = eval_fn(valid_dataloader, model, criterion, DEVICE, epoch)\n",
    "        print(f\"Loss: {valid_loss}  Acc: {valid_acc}  f1: {valid_f1}  \", end=\"\")\n",
    "\n",
    "        train_losses.append(train_loss)\n",
    "        train_accs.append(train_acc)\n",
    "        train_f1s.append(train_f1)\n",
    "        valid_losses.append(valid_loss)\n",
    "        valid_accs.append(valid_acc)\n",
    "        valid_f1s.append(valid_f1)\n",
    "\n",
    "        plot_training(train_losses, train_accs, train_f1s,\n",
    "                      valid_losses, valid_accs, valid_f1s,\n",
    "                      epoch, fold)\n",
    "        \n",
    "        best_loss = valid_loss if valid_loss < best_loss else best_loss\n",
    "        besl_acc = valid_acc if valid_acc > best_acc else best_acc\n",
    "        if valid_f1 > best_f1:\n",
    "            best_f1 = valid_f1\n",
    "            print(\"model saving!\", end=\"\")\n",
    "            torch.save(model.state_dict(), MODELS_DIR + f\"best_{MODEL_NAME}_{fold}.pth\")\n",
    "        print(\"\\n\")\n",
    "\n",
    "    return best_f1\n",
    "\n",
    "\n",
    "# training\n",
    "df = make_folded_df(TRAIN_FILE, NUM_SPLITS)\n",
    "f1_scores = []\n",
    "for fold in range(NUM_SPLITS):\n",
    "    print(f\"fold {fold}\", \"=\"*80)\n",
    "    f1 = trainer(fold, df)\n",
    "    f1_scores.append(f1)\n",
    "    print(f\"<fold={fold}> best score: {f1}\\n\")\n",
    "\n",
    "cv = sum(f1_scores) / len(f1_scores)\n",
    "print(f\"CV: {cv}\")\n",
    "\n",
    "lines = \"\"\n",
    "for i, f1 in enumerate(f1_scores):\n",
    "    line = f\"fold={i}: {f1}\\n\"\n",
    "    lines += line\n",
    "lines += f\"CV    : {cv}\"\n",
    "with open(f\"result/{MODEL_NAME}_result.txt\", mode='w') as f:\n",
    "    f.write(lines)\n",
    "\n",
    "\n",
    "# inference\n",
    "models = []\n",
    "for fold in range(NUM_SPLITS):\n",
    "    model = Classifier(MODEL_NAME)\n",
    "    model.load_state_dict(torch.load(MODELS_DIR + f\"best_{MODEL_NAME}_{fold}.pth\"))\n",
    "    model.to(DEVICE)\n",
    "    model.eval()\n",
    "    models.append(model)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "test_df = pd.read_csv(TEST_FILE)\n",
    "\n",
    "#p = re.compile(r\"<[^>]*?>|&amp;|[.,/'’\\\"”]\")\n",
    "#test_df['description'] = test_df['description'].map(lambda x: p.sub(\"\", x))\n",
    "#test_df['description'] = test_df['description'].map(lambda x: x.lstrip())\n",
    "\n",
    "\n",
    "test_df[\"labels\"] = -1\n",
    "test_dataset = make_dataset(test_df, tokenizer, DEVICE)\n",
    "test_dataloader = torch.utils.data.DataLoader(\n",
    "    test_dataset, batch_size=VALID_BATCH_SIZE, shuffle=False)\n",
    "\n",
    "with torch.no_grad():\n",
    "    progress = tqdm(test_dataloader, total=len(test_dataloader))\n",
    "    final_output = []\n",
    "\n",
    "    for batch in progress:\n",
    "        progress.set_description(\"<Test>\")\n",
    "\n",
    "        attention_mask, input_ids, labels = batch.values()\n",
    "        token_type_ids = None\n",
    "\n",
    "        outputs = []\n",
    "        for model in models:\n",
    "            output = model(input_ids, attention_mask, token_type_ids)\n",
    "            #output = output[0]\n",
    "            outputs.append(output)\n",
    "            \n",
    "        \n",
    "        outputs = sum(outputs) / len(outputs)\n",
    "        outputs = torch.softmax(outputs, dim=1).cpu().detach().tolist()\n",
    "        outputs = np.argmax(outputs, axis=1)\n",
    "\n",
    "        final_output.extend(outputs)\n",
    "\n",
    "submit = pd.read_csv(os.path.join(data_dir, \"submit_sample.csv\"), names=[\"id\", \"labels\"])\n",
    "submit[\"labels\"] = final_output\n",
    "submit[\"labels\"] = submit[\"labels\"] + 1\n",
    "\n",
    "\n",
    "# ============= 後処理 ==============================\n",
    "#re_labels = {1:1 , 2:3 , 3:4}\n",
    "#submit[\"labels\"] = submit[\"labels\"].map(re_labels)\n",
    "# ====================================================\n",
    "            \n",
    "try:\n",
    "    submit.to_csv(\"./output/Roberta_submission_cv{}.csv\".format(str(cv).replace(\".\", \"\")[:10]), index=False, header=False)\n",
    "except NameError:\n",
    "    submit.to_csv(\"./output/submission.csv\", index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b33c1e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
