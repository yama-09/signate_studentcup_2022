{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ddea38a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: NVIDIA GeForce RTX 3060\n",
      "fold 0 ================================================================================\n",
      "microsoft/deberta-base\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4183780972274e25b98e0335b5ef3c9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1212 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ec341b5dc0a4df1ad7a6f732d55f49b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/304 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-base were not used when initializing DebertaModel: ['lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.weight']\n",
      "- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddd9ab3693ca4718ae0ef18101195a7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/38 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before tensor([[-0.3064,  0.1744, -0.2730, -0.0332],\n",
      "        [-0.2750,  0.1110, -0.3240, -0.0197],\n",
      "        [-0.2901,  0.0783, -0.3251,  0.0356],\n",
      "        [-0.2687,  0.1055, -0.3041, -0.0051],\n",
      "        [-0.2587,  0.0940, -0.3287,  0.0090],\n",
      "        [-0.3206,  0.1507, -0.2212, -0.0589],\n",
      "        [-0.2767,  0.0953, -0.3350,  0.0240],\n",
      "        [-0.3994, -0.0881, -0.1003, -0.0198],\n",
      "        [-0.0244,  0.2096, -0.2291,  0.3723],\n",
      "        [-0.2784,  0.1101, -0.3283, -0.0126],\n",
      "        [-0.2612, -0.1264, -0.1257,  0.0375],\n",
      "        [-0.3324,  0.0861, -0.5246, -0.3097],\n",
      "        [-0.3383,  0.0805, -0.5027, -0.2923],\n",
      "        [-0.2474,  0.1706, -0.1527, -0.0178],\n",
      "        [-0.1933,  0.1291, -0.2186,  0.0362],\n",
      "        [-0.2612,  0.1153, -0.3293,  0.0030],\n",
      "        [-0.0122,  0.2016, -0.1225,  0.1999],\n",
      "        [ 0.0259, -0.0839, -0.0124,  0.3692],\n",
      "        [-0.2703,  0.0884, -0.3263, -0.0168],\n",
      "        [-0.3560,  0.0877, -0.5578, -0.3355],\n",
      "        [-0.3441,  0.0629, -0.5153, -0.2761],\n",
      "        [-0.2954,  0.1058, -0.3312,  0.0315],\n",
      "        [ 0.0919,  0.2702, -0.0027,  0.2250],\n",
      "        [-0.2676,  0.0845, -0.3509, -0.0134],\n",
      "        [-0.2877,  0.0908, -0.3203,  0.0049],\n",
      "        [-0.2965,  0.0872, -0.3339,  0.0398],\n",
      "        [-0.2809,  0.0893, -0.3260, -0.0121],\n",
      "        [-0.3383,  0.1074, -0.2523, -0.0522],\n",
      "        [-0.0285,  0.1970, -0.2428,  0.3891],\n",
      "        [-0.2617,  0.0792, -0.3046,  0.0048],\n",
      "        [-0.2751, -0.1279, -0.1446,  0.0450],\n",
      "        [ 0.1350,  0.2339, -0.0847,  0.2453]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "only integer tensors of a single element can be converted to an index",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_5620/4247994697.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    320\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfold\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mNUM_SPLITS\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    321\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"fold {fold}\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"=\"\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m80\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 322\u001b[1;33m     \u001b[0mf1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfold\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    323\u001b[0m     \u001b[0mf1_scores\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    324\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"<fold={fold}> best score: {f1}\\n\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_5620/4247994697.py\u001b[0m in \u001b[0;36mtrainer\u001b[1;34m(fold, df)\u001b[0m\n\u001b[0;32m    289\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    290\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 291\u001b[1;33m         \u001b[0mtrain_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_acc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_f1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDEVICE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    292\u001b[0m         \u001b[0mvalid_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid_acc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid_f1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0meval_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalid_dataloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDEVICE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    293\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Loss: {valid_loss}  Acc: {valid_acc}  f1: {valid_f1}  \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_5620/4247994697.py\u001b[0m in \u001b[0;36mtrain_fn\u001b[1;34m(dataloader, model, criterion, optimizer, scheduler, device, epoch)\u001b[0m\n\u001b[0;32m    147\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    148\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 149\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtoken_type_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    150\u001b[0m         \u001b[1;32mdel\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtoken_type_ids\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1183\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1184\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1185\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1186\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1187\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_5620/4247994697.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids)\u001b[0m\n\u001b[0;32m    119\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Before\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m         \u001b[0mweight\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m0.3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0.05\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0.3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0.35\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 121\u001b[1;33m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutput\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    122\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"After\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    123\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: only integer tensors of a single element can be converted to an index"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "import os\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import nlp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import AdamW, AutoModel, AutoTokenizer\n",
    "import re\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "# seeds\n",
    "SEED = 42\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "seed_everything(SEED)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    current_device = torch.cuda.current_device()\n",
    "    print(\"Device:\", torch.cuda.get_device_name(current_device))\n",
    "\n",
    "\n",
    "# config\n",
    "data_dir = os.path.join( \"input/\")\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "TRAIN_FILE = os.path.join(data_dir, \"train.csv\") # train.csv\n",
    "TEST_FILE = os.path.join(data_dir, \"test.csv\")\n",
    "MODELS_DIR = \"models/\"\n",
    "MODEL_NAME = 'microsoft/deberta-base' #deberta-base,deberta-v3-base,deberta-base-mnli\n",
    "TRAIN_BATCH_SIZE = 32\n",
    "VALID_BATCH_SIZE = 128\n",
    "NUM_CLASSES = 4\n",
    "EPOCHS = 5 # 10\n",
    "NUM_SPLITS = 5\n",
    "\n",
    "\n",
    "# dataset\n",
    "def make_folded_df(csv_file, num_splits=5):\n",
    "    df = pd.read_csv(csv_file)\n",
    "    \n",
    "    # ===================\n",
    "    p = re.compile(r\"<[^>]*?>|&amp;|\\\\[/'\\\"”]\")\n",
    "    df['description'] = df['description'].map(lambda x: p.sub(\"\", x))\n",
    "    df['description'] = df['description'].map(lambda x: x.lstrip())\n",
    "    # ===================\n",
    "    #df = df[df[\"jobflag\"] != 2].reset_index(drop=True)\n",
    "    #re_labels = {1:1 , 3:2 , 4:3}\n",
    "    #df[\"jobflag\"] = df[\"jobflag\"].map(re_labels)\n",
    "    # ===================\n",
    "    \n",
    "    df[\"jobflag\"] = df[\"jobflag\"] - 1\n",
    "    df[\"kfold\"] = np.nan\n",
    "    df = df.rename(columns={'jobflag': 'labels'})\n",
    "    label = df[\"labels\"].tolist()\n",
    "\n",
    "    skfold = StratifiedKFold(num_splits, shuffle=True, random_state=SEED)\n",
    "    for fold, (_, valid_indexes) in enumerate(skfold.split(range(len(label)), label)):\n",
    "        for i in valid_indexes:\n",
    "            df.iat[i,3] = fold\n",
    "    return df\n",
    "\n",
    "def make_dataset(df, tokenizer, device):\n",
    "    dataset = nlp.Dataset.from_pandas(df)\n",
    "    dataset = dataset.map(\n",
    "        lambda example: tokenizer(example[\"description\"],\n",
    "                                  padding=\"max_length\",\n",
    "                                  truncation=True,\n",
    "                                  max_length=128)) #128\n",
    "    dataset.set_format(type='torch', \n",
    "                       columns=['input_ids', 'attention_mask', 'labels'], \n",
    "                       device=device)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "# model\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, model_name, num_classes=4):\n",
    "        super().__init__()\n",
    "\n",
    "        self.bert = AutoModel.from_pretrained(model_name)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.linear = nn.Linear(768, num_classes)\n",
    "        nn.init.normal_(self.linear.weight, std=0.02)\n",
    "        nn.init.zeros_(self.linear.bias)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        output = self.bert(\n",
    "                input_ids = input_ids,\n",
    "                attention_mask = attention_mask,\n",
    "                return_dict=False\n",
    "                )\n",
    "        output = output[0]\n",
    "        output = output[:, 0, :]\n",
    "        output = self.dropout(output)\n",
    "        output = self.linear(output)\n",
    "        \n",
    "        #print(output.shape)\n",
    "        #print(output[:, 0, :].shape)\n",
    "        #print(output)\n",
    "        return output\n",
    "\n",
    "\n",
    "# training function\n",
    "def train_fn(dataloader, model, criterion, optimizer, scheduler, device, epoch):\n",
    "    \n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_corrects = 0\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "\n",
    "    progress = tqdm(dataloader, total=len(dataloader))\n",
    "\n",
    "    for i, batch in enumerate(progress):\n",
    "        progress.set_description(f\"<Train> Epoch{epoch+1}\")\n",
    "\n",
    "        if len(batch.values())==4:\n",
    "            attention_mask, input_ids, labels, token_type_ids = batch.values()\n",
    "        else:\n",
    "            attention_mask, input_ids, labels = batch.values()\n",
    "            token_type_ids = None\n",
    "        del batch\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(input_ids, attention_mask, token_type_ids)\n",
    "        del input_ids, attention_mask, token_type_ids\n",
    "        \n",
    "        \n",
    "        loss = criterion(outputs, labels)  # 損失を計算\n",
    "        \n",
    "        _, preds = torch.max(outputs, 1)  # ラベルを予測\n",
    "        del outputs\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        del loss\n",
    "        total_corrects += torch.sum(preds == labels)\n",
    "\n",
    "        all_labels += labels.tolist()\n",
    "        all_preds += preds.tolist()\n",
    "        del labels, preds\n",
    "\n",
    "        progress.set_postfix(loss=total_loss/(i+1), f1=f1_score(all_labels, all_preds, average=\"macro\"))\n",
    "\n",
    "    train_loss = total_loss / len(dataloader)\n",
    "    train_acc = total_corrects.double().cpu().detach().numpy() / len(dataloader.dataset)\n",
    "    train_f1 = f1_score(all_labels, all_preds, average=\"macro\")\n",
    "\n",
    "    return train_loss, train_acc, train_f1\n",
    "\n",
    "\n",
    "def eval_fn(dataloader, model, criterion, device, epoch):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_corrects = 0\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        progress = tqdm(dataloader, total=len(dataloader))\n",
    "        \n",
    "        for i, batch in enumerate(progress):\n",
    "            progress.set_description(f\"<Valid> Epoch{epoch+1}\")\n",
    "\n",
    "            if len(batch.values())==4:\n",
    "                attention_mask, input_ids, labels, token_type_ids = batch.values()\n",
    "            else:\n",
    "                attention_mask, input_ids, labels = batch.values()\n",
    "                token_type_ids = None\n",
    "            del batch\n",
    "\n",
    "            outputs = model(input_ids, attention_mask, token_type_ids)\n",
    "            del input_ids, attention_mask, token_type_ids\n",
    "            \n",
    "            #outputs = outputs[0]\n",
    "            \n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            del outputs\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            del loss\n",
    "            total_corrects += torch.sum(preds == labels)\n",
    "\n",
    "            all_labels += labels.tolist()\n",
    "            all_preds += preds.tolist()\n",
    "            del labels, preds\n",
    "\n",
    "            progress.set_postfix(loss=total_loss/(i+1), f1=f1_score(all_labels, all_preds, average=\"macro\"))\n",
    "\n",
    "    valid_loss = total_loss / len(dataloader)\n",
    "    valid_acc = total_corrects.double().cpu().detach().numpy() / len(dataloader.dataset)\n",
    "\n",
    "    valid_f1 = f1_score(all_labels, all_preds, average=\"macro\")\n",
    "\n",
    "    return valid_loss, valid_acc, valid_f1\n",
    "\n",
    "\n",
    "def plot_training(train_losses, train_accs, train_f1s,\n",
    "                  valid_losses, valid_accs, valid_f1s,\n",
    "                  epoch, fold):\n",
    "    \n",
    "    loss_df = pd.DataFrame({\"Train\":train_losses,\n",
    "                            \"Valid\":valid_losses},\n",
    "                        index=range(1, epoch+2))\n",
    "    loss_ax = sns.lineplot(data=loss_df).get_figure()\n",
    "    loss_ax.savefig(f\"figures/loss_plot_fold={fold}.png\", dpi=300)\n",
    "    loss_ax.clf()\n",
    "\n",
    "    acc_df = pd.DataFrame({\"Train\":train_accs,\n",
    "                           \"Valid\":valid_accs},\n",
    "                          index=range(1, epoch+2))\n",
    "    acc_ax = sns.lineplot(data=acc_df).get_figure()\n",
    "    acc_ax.savefig(f\"figures/acc_plot_fold={fold}.png\", dpi=300)\n",
    "    acc_ax.clf()\n",
    "\n",
    "    f1_df = pd.DataFrame({\"Train\":train_f1s,\n",
    "                          \"Valid\":valid_f1s},\n",
    "                         index=range(1, epoch+2))\n",
    "    f1_ax = sns.lineplot(data=f1_df).get_figure()\n",
    "    f1_ax.savefig(f\"figures/f1_plot_fold={fold}.png\", dpi=300)\n",
    "    f1_ax.clf()\n",
    "\n",
    "def trainer(fold, df):\n",
    "    \n",
    "    train_df = df[df.kfold != fold].reset_index(drop=True)\n",
    "    valid_df = df[df.kfold == fold].reset_index(drop=True)\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "    \n",
    "    print(MODEL_NAME)\n",
    "    \n",
    "    train_dataset = make_dataset(train_df, tokenizer, DEVICE)\n",
    "    valid_dataset = make_dataset(valid_df, tokenizer, DEVICE)\n",
    "\n",
    "    train_dataloader = torch.utils.data.DataLoader(\n",
    "        train_dataset, batch_size=TRAIN_BATCH_SIZE, shuffle=True\n",
    "    )\n",
    "    valid_dataloader = torch.utils.data.DataLoader(\n",
    "        valid_dataset, batch_size=VALID_BATCH_SIZE, shuffle=False\n",
    "    )\n",
    "\n",
    "    model = Classifier(MODEL_NAME, num_classes=NUM_CLASSES)\n",
    "    model = model.to(DEVICE)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=100000, gamma=1.0)\n",
    "    # ダミーのスケジューラー\n",
    "\n",
    "    train_losses = []\n",
    "    train_accs = []\n",
    "    train_f1s = []\n",
    "    valid_losses = []\n",
    "    valid_accs = []\n",
    "    valid_f1s = []\n",
    "\n",
    "    best_loss = np.inf\n",
    "    best_acc = 0\n",
    "    best_f1 = 0\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        train_loss, train_acc, train_f1 = train_fn(train_dataloader, model, criterion, optimizer, scheduler, DEVICE, epoch)\n",
    "        valid_loss, valid_acc, valid_f1 = eval_fn(valid_dataloader, model, criterion, DEVICE, epoch)\n",
    "        print(f\"Loss: {valid_loss}  Acc: {valid_acc}  f1: {valid_f1}  \", end=\"\")\n",
    "\n",
    "        train_losses.append(train_loss)\n",
    "        train_accs.append(train_acc)\n",
    "        train_f1s.append(train_f1)\n",
    "        valid_losses.append(valid_loss)\n",
    "        valid_accs.append(valid_acc)\n",
    "        valid_f1s.append(valid_f1)\n",
    "\n",
    "        plot_training(train_losses, train_accs, train_f1s,\n",
    "                      valid_losses, valid_accs, valid_f1s,\n",
    "                      epoch, fold)\n",
    "        \n",
    "        best_loss = valid_loss if valid_loss < best_loss else best_loss\n",
    "        besl_acc = valid_acc if valid_acc > best_acc else best_acc\n",
    "        if valid_f1 > best_f1:\n",
    "            best_f1 = valid_f1\n",
    "            print(\"model saving!\", end=\"\")\n",
    "            torch.save(model.state_dict(), MODELS_DIR + f\"best_{MODEL_NAME}_{fold}.pth\")\n",
    "        print(\"\\n\")\n",
    "\n",
    "    return best_f1\n",
    "\n",
    "\n",
    "# training\n",
    "df = make_folded_df(TRAIN_FILE, NUM_SPLITS)\n",
    "f1_scores = []\n",
    "for fold in range(NUM_SPLITS):\n",
    "    print(f\"fold {fold}\", \"=\"*80)\n",
    "    f1 = trainer(fold, df)\n",
    "    f1_scores.append(f1)\n",
    "    print(f\"<fold={fold}> best score: {f1}\\n\")\n",
    "\n",
    "cv = sum(f1_scores) / len(f1_scores)\n",
    "print(f\"CV: {cv}\")\n",
    "\n",
    "lines = \"\"\n",
    "for i, f1 in enumerate(f1_scores):\n",
    "    line = f\"fold={i}: {f1}\\n\"\n",
    "    lines += line\n",
    "lines += f\"CV    : {cv}\"\n",
    "with open(f\"result/{MODEL_NAME}_result.txt\", mode='w') as f:\n",
    "    f.write(lines)\n",
    "\n",
    "\n",
    "# inference\n",
    "models = []\n",
    "for fold in range(NUM_SPLITS):\n",
    "    model = Classifier(MODEL_NAME)\n",
    "    model.load_state_dict(torch.load(MODELS_DIR + f\"best_{MODEL_NAME}_{fold}.pth\"))\n",
    "    model.to(DEVICE)\n",
    "    model.eval()\n",
    "    models.append(model)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "test_df = pd.read_csv(TEST_FILE)\n",
    "\n",
    "# ================\n",
    "p = re.compile(r\"<[^>]*?>|&amp;|\\\\[/'\\\"”]\")\n",
    "test_df['description'] = test_df['description'].map(lambda x: p.sub(\"\", x))\n",
    "test_df['description'] = test_df['description'].map(lambda x: x.lstrip())\n",
    "# ================\n",
    "test_df[\"labels\"] = -1\n",
    "test_dataset = make_dataset(test_df, tokenizer, DEVICE)\n",
    "test_dataloader = torch.utils.data.DataLoader(\n",
    "    test_dataset, batch_size=VALID_BATCH_SIZE, shuffle=False)\n",
    "\n",
    "with torch.no_grad():\n",
    "    progress = tqdm(test_dataloader, total=len(test_dataloader))\n",
    "    final_output = []\n",
    "\n",
    "    for batch in progress:\n",
    "        progress.set_description(\"<Test>\")\n",
    "\n",
    "        attention_mask, input_ids, labels = batch.values()\n",
    "        token_type_ids = None\n",
    "\n",
    "        outputs = []\n",
    "        for model in models:\n",
    "            output = model(input_ids, attention_mask, token_type_ids)\n",
    "            #output = output[0]\n",
    "            outputs.append(output)\n",
    "            \n",
    "        \n",
    "        outputs = sum(outputs) / len(outputs)\n",
    "        outputs = torch.softmax(outputs, dim=1).cpu().detach().tolist()\n",
    "        outputs = np.argmax(outputs, axis=1)\n",
    "\n",
    "        final_output.extend(outputs)\n",
    "\n",
    "submit = pd.read_csv(os.path.join(data_dir, \"submit_sample.csv\"), names=[\"id\", \"labels\"])\n",
    "submit[\"labels\"] = final_output\n",
    "submit[\"labels\"] = submit[\"labels\"] + 1\n",
    "\n",
    "# ===================\n",
    "#re_labels = {1:1 , 2:3 , 3:4}\n",
    "#submit[\"labels\"] = submit[\"labels\"].map(re_labels)\n",
    "# ===================\n",
    "    \n",
    "try:\n",
    "    submit.to_csv(\"./output/Deberta_submission_cv{}.csv\".format(str(cv).replace(\".\", \"\")[:10]), index=False, header=False)\n",
    "except NameError:\n",
    "    submit.to_csv(\"./output/submission.csv\", index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9aa1ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"finish\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "842e411f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  0.8155509896245823"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be98b754",
   "metadata": {},
   "source": [
    "### コンペの方針\n",
    "- MLエンジニアの予測により精度を下げている可能性が大きい\n",
    "- MLエンジニアなしで予測を行い、後処理でMLエンジニアを予測していく"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
